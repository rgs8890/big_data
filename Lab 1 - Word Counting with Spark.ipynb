{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1vhL_tXQSfI2Q2DZL3fSLSyRaALvY3bMH","timestamp":1579808492341}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"gpuClass":"standard"},"cells":[{"cell_type":"markdown","metadata":{"id":"RKJWSvSKOl1c"},"source":["# Lab Sheet 1: PySpark Demo and Word Counting with Spark\n","\n","To get you started, we'll walk you through a bit of Colab specific Python and some PySpark code, and then we'll do the classic word count example, followed by some tasks for you to try.\n","\n","**Please run through the notebook cell by cell (clicking on the little icons left of the code, using 'run' above, or pressing 'shift-return' on the keyboard).**"]},{"cell_type":"markdown","metadata":{"id":"95B3FYvqPya6"},"source":["##Preliminaries: Preparing Colab and Spark\n","1.   When you open this notebook from the shared \"Big_Data\" folder, you **don't have write access**. When you save it, a **copy** will be created in the **folder \"Colab Notebooks\"**.\n","2.   The code below will **mount your Google Drive** as a **directory** in the file system of your vitual machine (running Linux). You will need to **authorise** this in a pop-up window that is opened automatically."]},{"cell_type":"code","metadata":{"id":"8edGFcfkPx50","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1738164116968,"user_tz":0,"elapsed":25733,"user":{"displayName":"Tillman Weyde","userId":"17945142824947980120"}},"outputId":"55fe30a0-02c6-4ed2-e804-575476f62df2"},"source":["# Load the Drive helper and mount\n","from google.colab import drive\n","\n","# This will prompt for authorization.\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","metadata":{"id":"GpO-PaoGUID4"},"source":["Next, we check if we can read the `Big_Data` folder. If the command below fails, go back to the shared [`Big_Data`]() folder and click on **\"Add to My Drive\"** in the folder menu."]},{"cell_type":"code","metadata":{"id":"_Bn28SFjUHC_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1738164117570,"user_tz":0,"elapsed":606,"user":{"displayName":"Tillman Weyde","userId":"17945142824947980120"}},"outputId":"3430c2d9-8b1a-4fe1-bfe0-a091977d8bef"},"source":["%ls \"/content/drive/My Drive/Big_Data/data/\""],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[0m\u001b[01;34m20_newsgroups\u001b[0m/  hamlet.txt  \u001b[01;34mlingspam_public\u001b[0m/  \u001b[01;34mspark\u001b[0m/\n","\u001b[01;34mfoodhygiene\u001b[0m/    \u001b[01;34mlibrary\u001b[0m/    \u001b[01;34mmovielens-small\u001b[0m/  stopwords2.txt\n"]}]},{"cell_type":"markdown","metadata":{"id":"fpUd_fHJRInX"},"source":["Next, we **install Spark** (may take a minute or two). This will need to be done **every time a new virtual machine is created**.\n"]},{"cell_type":"code","metadata":{"id":"2S9ShIHjSlDS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1738164254096,"user_tz":0,"elapsed":26987,"user":{"displayName":"Tillman Weyde","userId":"17945142824947980120"}},"outputId":"b17fa3b0-a62c-4a58-a2ad-cfc494299283"},"source":["# you can download the current verision of Spark from here:\n","# https://dlcdn.apache.org/spark/\n","# more info is available here: https://spark.apache.org\n","# We have already provided a current spark version in the Google drive,\n","# which works more quickly.\n","%cd\n","!tar -xzf \"/content/drive/My Drive/Big_Data/data/spark/spark-3.5.0-bin-hadoop3.tgz\" # unpacking\n","!apt-get install openjdk-8-jdk-headless -qq > /dev/null # installing java\n","\n","\n","import os # Python package for interaction with the operating system\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\" # tell the system where Java lives\n","os.environ[\"SPARK_HOME\"] = \"/root/spark-3.5.0-bin-hadoop3\" # and where spark lives\n","!pip install -q findspark # install helper package\n","import findspark # use the helper package\n","findspark.init() # to set up spark\n","%cd \"/content/drive/My Drive/Big_Data\""],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/root\n","/content/drive/MyDrive/Big_Data_Material/Big_Data\n"]}]},{"cell_type":"markdown","metadata":{"id":"rmJvtPxjOl1e"},"source":["## Part 1 - Demo: Apache Spark API with PySpark\n","\n","Basically, there are **2 main APIs** available in Apache Spark - **RDD** (Resilient Distributed Datasets) and **DataFrame** (extended by Dataset in Scala and Java). In this lab we will look at RDDs and Dataframes in Python.\n","\n","For **more information** on the **Spark framework** - visit (https://spark.apache.org).\n","For more information on the **Pyspark API** - visit (https://spark.apache.org/docs/latest/api/python/index.html)."]},{"cell_type":"markdown","metadata":{"id":"zmoNsdqQOl1f"},"source":["### 1) Access to Spark\n","\n","We start by cretaing a **SparkContext**, normally called **`sc`**.\n","We use that to create RDDs and a **SparkSession** object (for DataFrames), often just called **`spark`**."]},{"cell_type":"code","metadata":{"id":"12aW4ncAOl1h","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1738164380460,"user_tz":0,"elapsed":170,"user":{"displayName":"Tillman Weyde","userId":"17945142824947980120"}},"outputId":"a24aa524-a7d5-4db8-b4de-d4411e35fbc9"},"source":["import pyspark\n","# get a spark context\n","sc = pyspark.SparkContext.getOrCreate()\n","print(sc)\n","# get the context\n","spark = pyspark.sql.SparkSession.builder.getOrCreate()\n","print(spark)\n","print(spark.version)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["<SparkContext master=local[*] appName=pyspark-shell>\n","<pyspark.sql.session.SparkSession object at 0x7d69b54a6c90>\n","3.5.0\n"]}]},{"cell_type":"markdown","metadata":{"id":"ctb3qeAkOl1m"},"source":["### 2) RDD Creation\n","\n","There are **multiple** ways to **create RDDs**.\n","\n","One is to **parallelise** a **Python object** that exists in your driver process (i.e. the process that runs this notebook), which we will do in the next cell.\n","\n","Anoter way is to create an **RDD** is by referencing an **external dataset** such as a shared filesystem, HDFS, HBase, or any data source offering a Hadoop InputFormat. This is what we will be using later in this lab (further down)."]},{"cell_type":"code","metadata":{"id":"phwtMBs2Ol1p","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1738165406744,"user_tz":0,"elapsed":741,"user":{"displayName":"Tillman Weyde","userId":"17945142824947980120"}},"outputId":"4eaa8981-1aff-4ee6-a743-8c98730ed04f"},"source":["# Create an RDD from a Python object in this process (the \"driver\").\n","# The parallelize function  creating the \"numbers\" RDD\n","data = [1,2,3,4,5]\n","firstRdd = sc.parallelize(data)\n","print(\"RDD object info: \", firstRdd)\n","print(\"RDD element count: \", firstRdd.count())\n","print(\"RDD storage level: \", firstRdd.getStorageLevel())\n","print(\"RDD number of partitions: \", firstRdd.getNumPartitions())"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["RDD object info:  ParallelCollectionRDD[6] at readRDDFromFile at PythonRDD.scala:289\n","RDD element count:  5\n","RDD storage level:  Serialized 1x Replicated\n","RDD number of partitions:  2\n"]}]},{"cell_type":"markdown","metadata":{"id":"sx75RI9mOl1s"},"source":["This RDD lives now on as many worker machines as are available and as deemed useful by Spark. As we run Spark locally, there is only one machine.\n","\n","The 'object info' shows the type of RDD and its inetrnal ID (`ParallelCollectionRDD[4]`).\n","\n","The `.count()` method gives us the number of items which is 5, as expected.\n","\n","We can see that **`firstRDD`** is replicated once.\n","There are **2 partitions**, so we can have 2 parallel threads (which is the default on a single small machine like that provided in Colab)."]},{"cell_type":"markdown","metadata":{"id":"cDnZCdSTOl1t"},"source":["### 3) RDD operations\n","RDDs have two kinds of operations: ***Transformations*** and ***Actions***.\n","\n","***Transformations*** create a new RDD by applying a function to the items in the RDD. The function will be registered with the new RDD, but only be applied when an Action is triggered. This is called ***lazy evaluation***.\n","\n","***Actions*** produce some output from the data. An *Action* will trigger the execution of all *Transformations* registered with the RDD and its predecessors.\n","\n","Here are some examples:"]},{"cell_type":"code","metadata":{"id":"Q25kDkt9Ol1v","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1738165409278,"user_tz":0,"elapsed":194,"user":{"displayName":"Tillman Weyde","userId":"17945142824947980120"}},"outputId":"9398bdea-a740-4e62-d768-99a20fee344d"},"source":["def myfun(x):\n","  return x+3\n","rdd2 = firstRdd.map(myfun)\n","print(rdd2)\n","# nothing happened to far, as there has been no action"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["PythonRDD[8] at RDD at PythonRDD.scala:53\n"]}]},{"cell_type":"markdown","metadata":{"id":"QhIIytM9bSvm"},"source":["If the functions are short (one expression, to be exact), it can be more convenient to write a **lambda** expression, that creates an **anonymous function**."]},{"cell_type":"code","metadata":{"id":"IEtzNnm4bZfe","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1738165454138,"user_tz":0,"elapsed":189,"user":{"displayName":"Tillman Weyde","userId":"17945142824947980120"}},"outputId":"bc3dce90-14d6-43b8-da59-636c87616639"},"source":["rdd3 = firstRdd.map(lambda x:x+3) # this is the same as using myfun\n","print(rdd3)\n","# again, nothing happened to far, as there has been no action"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["PythonRDD[10] at RDD at PythonRDD.scala:53\n"]}]},{"cell_type":"markdown","metadata":{"id":"6xp2aTh8Ol15"},"source":["So far, the function we created with the lambda has not actually been executed.\n","\n","Next we use **`collect`**, which is an **action** and therefore triggers the execution.\n","**`collect`**returns the items in the RDD back into this local driver process in a Python array. See here for the documentation: [https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.collect.html#pyspark.RDD.collect](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.collect.html#pyspark.RDD.collect)"]},{"cell_type":"code","metadata":{"id":"tdaNRc0SOl15","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1738165456747,"user_tz":0,"elapsed":688,"user":{"displayName":"Tillman Weyde","userId":"17945142824947980120"}},"outputId":"d2c83be5-d005-4888-f312-71c3ed0e0246"},"source":["a = rdd2.collect()\n","print(a)\n","\n","b = rdd3.collect()\n","print(b)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[4, 5, 6, 7, 8]\n","[4, 5, 6, 7, 8]\n"]}]},{"cell_type":"markdown","metadata":{"id":"FXmtXG0hOl19"},"source":["As we can seee above, *myfun* (RDD2) and the *lambda x: x+3* (RDD3) have the same effect.\n","\n","Look here for more information about the functions provided by the RDD class: (https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.html#pyspark.RDD) ."]},{"cell_type":"markdown","metadata":{"id":"lUl3upk0Ol1-"},"source":["### 4) Dataframes\n","\n","**Dataframes** are a **more structured** form of storage than RDDs and similar to Pandas dataframes.  \n","\n","Let us see how to create and use dataframes. There are three ways of **creating a dataframe**:\n","1. from an existing RDD.\n","2. from an external data source, e.g., loading the data from JSON or CSV files.\n","3. programmatically specifying schema and data.\n","\n","Here is an example for option a). We use the *Row* class to create structured data rows."]},{"cell_type":"code","metadata":{"id":"hSIzFqAQOl1_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1738165533607,"user_tz":0,"elapsed":10249,"user":{"displayName":"Tillman Weyde","userId":"17945142824947980120"}},"outputId":"33e16285-9391-4efb-9b01-3837370e2903"},"source":["from pyspark.sql import Row\n","\n","dataList = [('Anne',21),('Bob',22),('Carl',29),('Daisy',36)] # our data as a list\n","rdd = sc.parallelize(dataList) # RDD from the list\n","peopleRDD = rdd.map(lambda x: Row(name=x[0], age=int(x[1]))) # RDD\n","peopleDF = spark.createDataFrame(peopleRDD)\n","print(peopleDF)\n","peopleDF.show()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["DataFrame[name: string, age: bigint]\n","+-----+---+\n","| name|age|\n","+-----+---+\n","| Anne| 21|\n","|  Bob| 22|\n","| Carl| 29|\n","|Daisy| 36|\n","+-----+---+\n","\n"]}]},{"cell_type":"markdown","source":["We can now apply SparkSQL  "],"metadata":{"id":"2ciDfuB3vYCV"}},{"cell_type":"code","source":[],"metadata":{"id":"zqGhONnsvXgf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vsGAOxX2Ol2G"},"source":["## Part 2: Classic Word Count\n","\n","We will now do the **classic word count example** for the MapReduce pattern.\n","\n","We will apply it to the text of Sheakespeare's play *Hamlet*. To access that, you should have added the `Big_Data` folder to your own Google Drive. Double check that this part of the **preliminaries** section above was executed correctly, otherwise go back and fix this first."]},{"cell_type":"markdown","metadata":{"id":"-eWjGlkbOl2H"},"source":["### 1) Load the data\n","First we need to load the **text into an RDD** (the second method of creating an RDD as mentioned above).\n","\n","We need to specify the path, and we can read directly from the shared Big_Data directory."]},{"cell_type":"code","metadata":{"id":"kzdsuhxtOl2H"},"source":["filepath = \"/content/drive/My Drive/Big_Data/data/hamlet.txt\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_5q8UiCtOl2J"},"source":["You can read the file into an RDD with **`textFile`**. The RDD then contains as items the **lines of the text**. **`take(3)`** then gives us the first 3 lines.  "]},{"cell_type":"code","metadata":{"id":"OM_vQDsGOl2J","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1738166090717,"user_tz":0,"elapsed":413,"user":{"displayName":"Tillman Weyde","userId":"17945142824947980120"}},"outputId":"f517b56f-64bb-4a00-afb4-585decbc4e05"},"source":["lineRDD = sc.textFile(filepath)\n","lineRDD.take(3)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Project Gutenberg Etext of Hamlet by Shakespeare',\n"," \"PG has multiple editions of William Shakespeare's Complete Works\",\n"," '']"]},"metadata":{},"execution_count":20}]},{"cell_type":"markdown","metadata":{"id":"ab6vKwgvOl2L"},"source":["### 2) Split lines into words\n","\n","In order to count the words, we need to split the lines into words. We can do that using the **`split`** function of the Python String class to separate at each space.\n","\n","The map function replaces each RDD item with a new one. In this case, our **`lambda`** returns an array of words (produced by `split(' ')`). However, we want to create one RDD item per word. Therefore, we need to use a function called **`flatMap`** that creates a new RDD item for every array item  returned by the lambda function.  "]},{"cell_type":"code","metadata":{"id":"WwI2cDlSOl2L","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1738166202024,"user_tz":0,"elapsed":420,"user":{"displayName":"Tillman Weyde","userId":"17945142824947980120"}},"outputId":"dec3e364-e4e4-4ce8-958d-5833b9a0e764"},"source":["wordRDD = lineRDD.flatMap(lambda x: x.split(' '))\n","wordRDD.take(10)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Project',\n"," 'Gutenberg',\n"," 'Etext',\n"," 'of',\n"," 'Hamlet',\n"," 'by',\n"," 'Shakespeare',\n"," 'PG',\n"," 'has',\n"," 'multiple']"]},"metadata":{},"execution_count":22}]},{"cell_type":"markdown","metadata":{"id":"jDuHh_ocOl2N"},"source":["We  then **map** the words to **tuples** of the form *(word, 1)*."]},{"cell_type":"code","metadata":{"id":"0PXZXAGpOl2O"},"source":["word1RDD = wordRDD.map(lambda x: (x, 1))\n","word1RDD.take(3)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5BplN07GOl2P"},"source":["### 3) Count by reducing\n","For Spark, the first part in each tuple is the '**key**', the second part is  the '**value**'). Now we can use **`reduceByKey()`**. It reduces all values for the same key to one value, using the provided function. In this case, we create a **lambda function**, that **adds the given numbers**. Sine for every occurence of a word as key we have a 1 value, the addition gives the number of occurences per word."]},{"cell_type":"code","metadata":{"id":"HRFaQARsOl2Q"},"source":["wordCountRDD = word1RDD.reduceByKey(lambda x,y: x+y )\n","wordCountRDD.take(3)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"G38rUrlVOl2S"},"source":["### 4) Filtering\n","\n","There are many empty strings returned by the splitting. We can remove them by **filtering**.\n","**`filter()`** keeps only the items where the test function (specified by our lambda) returns `true`."]},{"cell_type":"code","metadata":{"id":"ZQ9mBefpOl2S"},"source":["wordFilteredRDD = wordRDD.filter(lambda x: len(x)>0)\n","word1RDD = wordFilteredRDD.map(lambda x: (x, 1))\n","wordCountRDD = word1RDD.reduceByKey(lambda x,y: x+y )\n","wcList = wordCountRDD.collect()\n","print(wcList[1:5])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oyubLNalOl2W"},"source":["## Part 3: Tasks for you to work on\n","\n","Based on the examples above, you can now try and write some code yourself.  Look for the lines starting with **>>>**. You neeed to fix them by writing your own code."]},{"cell_type":"markdown","metadata":{"id":"bubScba3vLNG"},"source":["### 0) Warm-up: Take a shortcut\n","\n","We can take a shortcut and use a **ready-made function** **`countByValue()`**, which does the same as we did above without us specifying a lambda for addition (https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.countByValue)."]},{"cell_type":"code","metadata":{"id":"bTJdSqcMveUt"},"source":["wordFilteredRDD = wordRDD.filter(lambda x: len(x)>0)\n",">>> wcList = # your code goes here, use countByValue on wordFilteredRDD to create wcList.\n","print(wcList)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4LY4f9UwOl2W"},"source":["### 1) Better splitting\n","\n","Currently our 'words' can contain punctuation, because only spaces are removed. A better way to split is using regular expressions  [Python's 're' package](https://docs.python.org/3.5/library/re.html?highlight=regular%20expressions). **`re.split('\\W+', 'my. test. string!')`** does a good job. Try it out below by fixing the line that starts with '>>>'."]},{"cell_type":"code","metadata":{"id":"wuQDbhOAOl2X"},"source":["import re\n",">>> wordRDD = lineRDD.flatMap(lambda x: ...) # apply re.split('\\W+', string) here\n","wordFilteredRDD = wordRDD.filter(lambda x: len(x)>0) # filtering\n","wordFilteredRDD.take(3)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ktBnkI5TOl2Y"},"source":["### 2) Use lower case\n","\n","**Convert all strings** to lower case (using **`.lower()`** provided by the Python string class), so that 'Test' and 'test' count as the same word. Package it into one a tuple of the form (word,1) in the same call."]},{"cell_type":"code","metadata":{"id":"DBqa2EQjOl2Y"},"source":[">>> wordLowerRDD = wordFilteredRDD.map(lambda x: ... )\n","wordLowerRDD.take(3)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WIVwV56lOl2a"},"source":["word1LowerRDD = wordLowerRDD.map(lambda x: (x, 1))\n","wordCountLowerRDD = word1LowerRDD.reduceByKey(lambda x,y: x+y) # we can now get better word count results\n","wordCountLowerRDD.take(3)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L7jgnYSVOl2b"},"source":["### 3) Filter rare words\n","\n","Add a filtering step call **remove all words with less than 5 occurrences**. This can be useful to if you want to identify common topics in documents, where very rare words can be misleading."]},{"cell_type":"code","metadata":{"id":"CTd9S_y6Ol2b"},"source":["# the trick here is to apply the lambda only to the second part of each item, i.e. x[1]\n",">>> freqWordsRDD = wordCountRDD.filter(lambda x:  ... ) # tip: filter keeps the elements for which the lambda function returns true\n","freqWordsRDD.take(3)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IypcO9SPOl2d"},"source":["### 4) List only stopwords\n","\n","**Stopwords** are frequent words that are not topic-specifc (articles, pronouns, ...). Removing stopwords can be useful in recognising the topic of a document. Removing non-stopwords can be useful for recognising the style of an author.\n","\n","Below is a small list of stopwords. Filter so that you keep the tuples where the **first element is in the stopword list**."]},{"cell_type":"code","metadata":{"id":"T85Nx6ZuOl2e"},"source":["stopWordList = ['the','a','in','of','on','at','for','by','I','you','me']\n",">>> stopWordsRDD = freqWordsRDD.filter(lambda x:  ) # the 1st part of the tuple should be in the list"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gKVS18B9Ol2f"},"source":["We can now list the results (there are only a few words."]},{"cell_type":"code","metadata":{"id":"JsX57MXHOl2g"},"source":["output = stopWordsRDD.collect()\n","for (word, count) in output:\n","    print(\"%s: %i\" % (word, count))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OQcQSKw0Ol2k"},"source":["## Reading\n","\n","Read chapter 1 of Lescovec et al (2019), \"Mining of Massive Datasets\", and work out the answers to exercise 1.2.1 on page 7 and 1.3.1 and 1.3.2 on page 15. If you have time, start reading chapter 2."]},{"cell_type":"markdown","metadata":{"id":"XBMQj71VOl2k"},"source":["## Spark @home (optional)\n","\n","You can try and install Spark on your own laptop or desktop, using the instructions provided on Moodle."]}]}