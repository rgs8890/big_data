{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xs3o00tAfXhD"
   },
   "source": [
    "# Lab Sheet 9, Spark Partitioning and Caching\n",
    "\n",
    "These tasks are for working in the lab session and during the week. \n",
    "\n",
    "We'll work with partitions, and partitioning functions and persistence. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9bzKmDsjQavN"
   },
   "source": [
    "\n",
    "The usual preparations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RVWRS7O6fXhG"
   },
   "outputs": [],
   "source": [
    "# Load the Drive helper and mount\n",
    "from google.colab import drive\n",
    "\n",
    "# This will prompt for authorization.\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "#install spark\n",
    "%cd\n",
    "!apt-get update -qq\n",
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "!tar -xzf \"/content/drive/My Drive/Big_Data/data/spark/spark-3.5.0-bin-hadoop3.tgz\"\n",
    "!pip install -q findspark\n",
    "\n",
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/root/spark-3.5.0-bin-hadoop3\"\n",
    "%cd /content\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "# get a spark context\n",
    "sc = pyspark.SparkContext.getOrCreate()\n",
    "print(sc)\n",
    "# and a spark session\n",
    "spark = pyspark.sql.SparkSession.builder.getOrCreate()\n",
    "print(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Jtq500YSxe5"
   },
   "source": [
    "## Task 1 Standard Partitioning\n",
    "\n",
    "We'll start by setting up an RDD with text as we did a few times before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "43sVuHNAXYHE"
   },
   "outputs": [],
   "source": [
    "rddLines = sc.textFile(\"/content/drive/My Drive/Big_Data/data/hamlet.txt\")\n",
    "rddLines.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8q2Va4hcXjGQ"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "rddWords = rddLines.flatMap(lambda c: re.split('\\W+',c)).filter(lambda w: len(w)>0)\n",
    "rddWords.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BPvAQaDNtMkT"
   },
   "outputs": [],
   "source": [
    "rddWordsW1 = rddWords.map(lambda w: (w,1))\n",
    "rddWordsW1.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VNsDHfp-kZwn"
   },
   "source": [
    "### a) find out the number of partitions in the rddWords and rddLines. Repartition to 10 partitions. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X06-2pyyTuZN"
   },
   "outputs": [],
   "source": [
    "print(rddWords ... ) >>> your code here, get the number of partitions\n",
    "rddWordsRp = >>> repartition to 10 partitions \n",
    "print(rddWordsRp ... ) >>> get the number of partitions again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_0tG_gZmficS"
   },
   "outputs": [],
   "source": [
    "print(rddLines ...) >>> same procedure as above.\n",
    ">>>\n",
    ">>>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wJb8jyQMfV06"
   },
   "source": [
    "b) print the size of each partition using glom and a map with a lambda (and collect). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qu9rFZ6WfTsC"
   },
   "outputs": [],
   "source": [
    "rddWordsRp >> use glom and map with a lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZJoTTdVAqIs6"
   },
   "outputs": [],
   "source": [
    "rddLines >>> like above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MwA5T6NkgDon"
   },
   "source": [
    "### c) now use [mapPartitions](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.mapPartitions.html#pyspark.RDD.mapPartitions) to print the sizes of the partitions.\n",
    "\n",
    "You need to provide a mapping function to mapPartitions (can be a lambda). The argument that the mapping function will get is an iterator. Iterators doen't have a function for counting their content. The fastest way to count the items in an iterator `iter` is to cast it to  `tuple(iter)` and apply `len`. The safest return type for the mapping function is a list `[]` wrapped around the value.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GZWEWuROrlZO"
   },
   "outputs": [],
   "source": [
    "rddWords >>> your code here, see instructions above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i3LdsuWThaZK"
   },
   "source": [
    "### d) use [mapPartitionsWithIndex](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.mapPartitionsWithIndex.html) to list the partition lengths together with the index for the partition. \n",
    "\n",
    "The solution is similar to 1c), but you need to accept two arguments in the mapping functions, index and partition iterator. Put the index and partition length  together in a tuple, and that still needs to be wrapped by  a list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6DkkkrBbccs1"
   },
   "outputs": [],
   "source": [
    "rddWords <<< your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PNr0gVkwiUUY"
   },
   "source": [
    "### e) for fun: see what `coalesce` does (it's not supposed to be used for increasing partition numbers). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LkUtHynpm7NO"
   },
   "outputs": [],
   "source": [
    "rddLinesRp = rddLines.coalesce(10)\n",
    "rddLinesRp.glom().map(lambda l: len(l)).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Et7YMmC4p8rZ"
   },
   "source": [
    "## 2) Custom Partitioning\n",
    "\n",
    "### a) Create a partitioning sorted by the length of the lines. \n",
    "\n",
    "This is a relevant usecase for training deep networks with text for NLP applications. \n",
    "There, strings of similar lengths should be kept together, to make batch training efficient. \n",
    "\n",
    "You start by mapping the the (w,1) tuples to the length of `w`. Then you calculate the minimal and maximal lenghth, which enables you to determine the number of partitions you need. \n",
    "\n",
    "Then you can map the length to partitions in a lambda and apply it with [`partitionBy`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.partitionBy.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jR1jJ2NFqBh3"
   },
   "outputs": [],
   "source": [
    "rddWordsW1 >>> map to (w,1) structure \n",
    "rddWordsWl >> map to word length\n",
    ">>> get min (offset) and max calculate `numPart` as the difference.\n",
    "print(\"offset: \",offset, \" numPart: \", numPart)\n",
    "rddWordsPb >>> use offset and numPart in partitionBy\n",
    "rddWordsPb >>> show partition sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jSa1cS9Ws0uN"
   },
   "source": [
    "b) You should see some skew in the partitioning (i.e. different partition sizes). Here a more specialised mapping of lenght values to partition indexes can help to counterbalance. Try to define and implement a mapping that reduces skew."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IWQ7eZAws0IE"
   },
   "outputs": [],
   "source": [
    ">>> create your counterbalancing partitioning function\n",
    "def lp(i):\n",
    ">>>    \n",
    "rddWordsPb = rddWordsW1.partitionBy(...) >>>\n",
    "rddWordsPb >>> show partition sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-YbyuC373T8_"
   },
   "source": [
    "### c) Create a partitioning by the first letter of a word, lower case, with [`partitionBy`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.partitionBy.html).\n",
    "\n",
    "For this partitioning, we first remove all strings starting with non-letter characters. For this, use `s[0]` to get the first character in a string (which is interestingly still  a string type) and filter with [`isalpha`](https://docs.python.org/3/library/stdtypes.html?highlight=isalpha#bytearray.isalpha)). \n",
    "\n",
    "Then, we need to identify the range of the characters. For that, we map all (strings,1) items to the first character of the string. Then apply [`lower`](https://docs.python.org/3/library/stdtypes.html?highlight=lower#str.lower) and get the character encoding (i.e. the unicode number) with [`ord`](https://docs.python.org/3/library/functions.html?highlight=ord#ord). \n",
    "\n",
    "Then we calculate the min and max of the unicode numbers. With these values, we can map the initial characters to the 25 letters of the Latin alphabet (a-z) in our partition function and apply `partitionBy`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o56w4jRmk6NT"
   },
   "outputs": [],
   "source": [
    "rddWordsW1 = rddWords ... >>> filter out non strings starting with non-letters\n",
    "rddWordsWl = rddWordsW1 >>> get unicode numbers\n",
    ">>> get min (offset) and max calculate `numPart` as the difference.\n",
    "print(\"offset: \",offset, \" numPart: \", numPart)\n",
    "rddWordsPb = rddWordsW1.partitionBy( ... ) <<< apply\n",
    "rddWordsPb >>> show sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PWQgDP19nP3-"
   },
   "source": [
    "### d) Show an example for each partition. \n",
    "\n",
    "We reuse the approach from 1d) here. The goal is to not just show the partition index and size, but also the letter with which the words in the partitions begin (or the whole word). \n",
    "\n",
    "You will need to write a named mapping function (i.e. defined with `def`, not in a lambda), which takes the partition index and the iterator, extracts and example word and the partition length, and then returns index, length and the work together in a tuple wrapped in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RBZ9N6udnezF"
   },
   "outputs": [],
   "source": [
    "#>>> define your mapping function\n",
    "def f(i,p):\n",
    ">>>\n",
    "rddWordsPb.mapPartitionsWithIndex(f).collect() # apply f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mY6wJUDnrxlP"
   },
   "source": [
    "## 3) Optimize with caching\n",
    "\n",
    "### a)\n",
    "Take the code from 2b) and identify which rdd should be cached to improve efficiency. Explain why, and how caching could help. Add the relevant code. \n",
    "\n",
    "### b) \n",
    "Time the code with and without caching using the `%%time` magic. The gains are not massive in this example, but you can try with more data to see more effect. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fOlt-jousDZn"
   },
   "outputs": [],
   "source": [
    "#>>> apply caching, take the time\n",
    "rddWordsW1 = rddWords.filter(lambda w: w[0].isalpha()).map(lambda w: (w,1))\n",
    "rddWordsWl = rddWordsW1.map(lambda w1: ord(w1[0][0].lower()))\n",
    "offset = rddWordsWl.min()\n",
    "numPart = rddWordsWl.max() - offset \n",
    "rddWordsPb = rddWordsW1.partitionBy(numPart,lambda w: ord(w[0].lower())-offset)\n",
    "rddWordsPb.glom().map(lambda l: len(l)).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ORMGpmc8x7o0"
   },
   "source": [
    "# Extra Tasks\n",
    "\n",
    "## 4) Implement a Bloom filter in plain Python. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m5V-xuC0YJF6"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "\n",
    "# The size of the array \n",
    "vSize = 10000\n",
    "v = [0]*vSize # the array for the Bloom filter\n",
    "\n",
    "kmax=100 # maximal number of hash-functions\n",
    "# The random parameters for universal hashing\n",
    "a = []\n",
    "b = []\n",
    "# use a for-loop to append kmax random numbers to a and b \n",
    ">>> for ... \n",
    ">>> \t... random.randint(1,vSize)\n",
    "\n",
    "def uhash(x,j):\n",
    "\t\"\"\" provides a hash of argument x based on the parameters in 'a' and 'b', \n",
    "\t as identified by j, based on universal hashing \n",
    "\t\"\"\"\n",
    ">>>\t implement universal hashing here with a[j] as 'a', b[j] as 'b', 433494437 as 'N'\n",
    ">>>\t mod is '%' in Python\n",
    "\n",
    "\n",
    "def fillArray(lst): \n",
    "\t\"\"\" fills the Bloom filter array with a list of items to hash \"\"\"\n",
    "\tfor x in lst: # for all items\n",
    ">>>\t\tfor ... : # iterate through all k hash functions\n",
    "\t\t\tidx = uhash(hash(x),i) % vSize # get the hashed position\n",
    ">>>\t\t\tv.... # and set position idx in v to 1\n",
    "\n",
    "\n",
    "def checkItem(itm):\n",
    "\t\"\"\" checks whether an item is in the set, only true if all hash addresses have '1's. \n",
    "\t\tmay produce false positives, because of collisions, though.\n",
    "\t\"\"\"\n",
    "\tfor i in range(k):\n",
    "\t\tidx = uhash(hash(itm),i) % vSize\n",
    ">>>\t\tif ... : # if array 'v' does not have a 1 at position 'idx'\n",
    "\t\t\treturn False\n",
    "\treturn True # return true only if all positions had a 1\n",
    "\n",
    "\n",
    "k=2 # number of hash functions used, you can experiment with the effect\n",
    "print('Using ',k,' hash function(s)')\n",
    "# read the list of items to look for, e.g. the stopwords list.\n",
    "with open(\"/data/BigDataMaterials/stopwords2.txt\") as f:\n",
    "\tstpWrdLst = f.readlines()\n",
    "\t# ... and build the array\n",
    "stpWrdLst = map(str.strip,stpWrdLst) # get rid of whitespace\n",
    "fillArray(stpWrdLst)\n",
    "# Then get the data to scan\n",
    "with open(\"/data/tempstore/library/emma.txt\") as f:\n",
    "\ttext = f.read()\n",
    "\ttextList = re.split(\"\\W+\",text)\n",
    "print('read ',len(textList),' words in text')\n",
    "# and check for hits with the Bloom filter\n",
    "import time # take the time\n",
    "start = time.time()\n",
    "count = 0 # count the stopwords\n",
    "for w in textList:\n",
    "\tif checkItem(w):\n",
    "\t\tcount = count + 1\n",
    "print(count, ' potential stopwords found in hamlet with Bloom filtering')\n",
    "end = time.time()\n",
    "print(end - start, \"seconds used for Bloom filtering\")\n",
    "# and compare to  linear search, as used by Python's 'in' operator, \n",
    "# which is more exact but has higher complexity\n",
    "start = time.time()\n",
    "count = 0\n",
    "for w in textList:\n",
    "\tif w in stpWrdLst:\n",
    "\t\tcount = count+1\n",
    "print(count, \" actual stopwords found in hamlet with linear search\")\n",
    "end = time.time()\n",
    "# output should look similar to this:\n",
    "# ('Using ', 2, ' hash function(s)')\n",
    "# ('read ', 165127, ' words in text')\n",
    "# (100737, ' potential stopwords found in hamlet with Bloom filtering')\n",
    "# (0.2430589199066162, 'seconds used for Bloom filtering')\n",
    "# (100734, ' actual stopwords found in hamlet with linear search')\n",
    "# (0.5513789653778076, 'seconds used for linear search')\n",
    "\n",
    "print(end - start, \"seconds used for linear search\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gJraZbwwNhk9"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Lab 9a (session 7, 25 March) - Partitioning and Caching.ipynb",
   "private_outputs": true,
   "provenance": [
    {
     "file_id": "13q4AyuGkgL-2NYhrbI-vkd0jhpwlFKzM",
     "timestamp": 1648151339590
    },
    {
     "file_id": "1zeoKX0tcLd3RIxhy5ksigZ6cF9_1Z1kB",
     "timestamp": 1648128287482
    },
    {
     "file_id": "1WWl2h4gf2QqipJP1faz48iMVyN9sJuAc",
     "timestamp": 1581610350561
    },
    {
     "file_id": "1AVbECIUnJcjz-blM7XeFNeETJQSnwpyw",
     "timestamp": 1581609108024
    },
    {
     "file_id": "1oUu05Wpzxx1VaWFtdwC2NnSoM1lsdgyQ",
     "timestamp": 1580399768359
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
