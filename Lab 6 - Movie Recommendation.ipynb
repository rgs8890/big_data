{"cells":[{"cell_type":"markdown","metadata":{"id":"gw8sWRdFqtYo"},"source":["# Lab Sheet 6: Movie recommendation using ALS in Spark\n","\n","These tasks are for working in the lab session and during the week. We will use Alternating Least Squares (ALS) in Spark to recommend movies and explore the outcome of a cross-validation to find its optimal parameter settings."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RrEXZtlCqztr"},"outputs":[],"source":["# Load the Drive helper and mount\n","from google.colab import drive\n","\n","# This will prompt for authorization.\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2mumNmd3q1x1"},"outputs":[],"source":["#install spark\n","%cd\n","!apt-get update -qq\n","!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n","!tar -xzf \"/content/drive/My Drive/Big_Data/data/spark/spark-3.5.0-bin-hadoop3.tgz\"\n","!pip install -q findspark\n","\n","import os\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n","os.environ[\"SPARK_HOME\"] = \"/root/spark-3.5.0-bin-hadoop3\"\n","%cd /content\n","import findspark\n","findspark.init()\n","\n","import pyspark\n","# get a spark context\n","sc = pyspark.SparkContext.getOrCreate()\n","print(sc)\n","# and a spark session\n","spark = pyspark.sql.SparkSession.builder.getOrCreate()\n","print(spark)"]},{"cell_type":"markdown","metadata":{"id":"6ysksD8nSYit"},"source":["## Task 1) Read the data into a DataFrame\n","\n","We will use a dataset of movie ratings, consisting of four columns separated by `::` A sample is displayed below. The first column contains `userId`s, the second `movieId`s, the third the `rating` of the user for that movie and the last column is a `timestamp`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iTFca7ZYLY9x"},"outputs":[],"source":["!head -n 5 \"/content/drive/MyDrive/Big_Data/data/movielens-small/sample_movielens_ratings.txt\""]},{"cell_type":"markdown","metadata":{"id":"j3mEJCXiqtYr"},"source":["**Read** the data, split into **tokens** and create a **structured DataFrame**. For low level tasks like splitting strings, we use an RDD where we can apply a `map` function.\n","\n","See here for how to create a **new Row object**:\n","https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.Row.html\n","When creating a new Row object, it is helpful to ensure that the elements in the row have a specific type. You can achive this by casting/converting a variable to a type, e.g. with int() https://docs.python.org/3.7/library/functions.html#int or float() https://docs.python.org/3.7/library/functions.html#float or str() https://docs.python.org/3.7/library/stdtypes.html#str .\n","\n","and here for **random split**ting of a DataFrame\n","https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.randomSplit.html\n","\n","An 80%-20% train/test set split is common."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OIqQ73oBqtYt"},"outputs":[],"source":["from pyspark.sql import Row\n","# the imports are used for creating the data frame\n","\n","# this gets us an RDD. (could also be done with RDD.textFile in this case)\n","lines = spark.read.text(\"/content/drive/MyDrive/Big_Data/data/movielens-small/sample_movielens_ratings.txt\").rdd\n","# the RDD items now look like this: Row(value=\"1::2::3::123465769\")\n","#now split the lines at the '::'\n",">>>parts = lines.map(lambda row: row.value ... ) # <<< split the row value using Python's split function for strings\n",">>>ratingsRDD = parts.map( ... ) # <<< create a new Row object with userId as int, movieId  as int, rating as float, timestamp as int\n","ratings = spark.createDataFrame( ratingsRDD ) # create a dataframe from an RDD of Row objects\n","ratings.createOrReplaceTempView('ratings') # register the DataFrame so that we can use it with Spark SQL.\n",">>>(training, test) =  ... #<<< create a random split into test and training set from the dataframe\n","print(training.show(5)) # just for testing, should show five columns\n","print(training.count()) # just for testing, should be around 1200"]},{"cell_type":"markdown","metadata":{"id":"E-amZql8qtY2"},"source":["## Task 2) Create a baseline recommendation\n","\n","Now take a very **simple estimate** as the baseline: calculate the **mean of all ratings**.    \n","\n","The average can be calculated with the **SQL `AVG` command**, within an SQL `SELECT` statement. If you replace selected column, e.g. `rating`, with `AVG(rating)`, the returned DataFrame will contain only 1 row. You can then access the contents of the row by its name, here as `row['avg(rating)']` (avg needs to be lower case here).\n","\n","Then calculate the **squared error** with respect to the average as predictor. You can again use the SQL `AVG` command on the error for the mean of the squared errors.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mDILZXRIqtY5"},"outputs":[],"source":["# select the average of the 'rating' entries from 'ratings' DF\n",">>>SQL1 = ' ... ' #<<<\n","row = spark.sql(SQL1).collect()[0] # get the single row with the result\n","print('row', row)\n",">>>meanRating = ... # access Row as a dictionary\n","print('meanRating', meanRating)\n","\n",">>>se_rdd = test.rdd.map( ... ) #<<< get the the squared error (difference to the average) using Python pow()\n","se_df = spark.createDataFrame(se_rdd) # create a data frame\n",">>>se_df.createOrReplaceTempView( ... ) #<<< Register with the SQL system (choose a name)\n","print('se_df', se_df)\n","\n","# get the average squared error\n",">>>SQL2 = '...' #<<< Use the name here that you chose 4 lines above\n","row = spark.sql(SQL2).collect()[0]\n",">>>meanSE = ... #<<< access Row as a dictionary\n","from math import sqrt\n",">>>print('RMSE', ...) #<<< calculate the root of the meanSE using Python's sqrt function"]},{"cell_type":"markdown","metadata":{"id":"RPkw5oTDqtY9"},"source":["## Task 3) Find the best parameters for ALS by cross-validation on the training set\n","\n","Now **create** an **ALS estimator** and a **parameter grid** to explore different values for the `rank` and `regParam` parameter of the ALS. Then build a **cross-validator** to train the model and optimise the parameters.\n","\n","Here is the doc for the **ALS class**:\n","https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.recommendation.ALS.html#pyspark.ml.recommendation.ALS\n","\n","Here it is for the **ParamGridBuilder**:\n","https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.tuning.ParamGridBuilder.html#pyspark.ml.tuning.ParamGridBuilder\n","\n","See here for more information about the **RegressionEvaluator**:\n","https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.evaluation.RegressionEvaluator.html#pyspark.ml.evaluation.RegressionEvaluator"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uBUBR7EtqtY9"},"outputs":[],"source":["from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n","from pyspark.ml.evaluation import RegressionEvaluator\n","from pyspark.ml.recommendation import ALS\n","\n","# Build the recommendation model using ALS on the training data\n","als = ALS(maxIter=5, rank=5, regParam=0.1, userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\")\n","\n","# build a parameter grid for rank and regParam.\n",">>>paramGrid = ParamGridBuilder() \\\n","    .addGrid(...) \\\n","    .addGrid(...).build() # <<<< For a first test, use just one value per parameter\n","\n","# set up a regression evaluater evaluating RMSE\n",">>>regEval = RegressionEvaluator( ... ) # <<<<\n","\n","# set up a cross validator with the als, paramGrid and regEval\n",">>>crossVal = CrossValidator( ... numFolds=3) # <<<<\n","\n","print('starting cross-validation')\n","cvModel = crossVal.fit(training)\n","print('finished cross-validation')"]},{"cell_type":"markdown","metadata":{"id":"JxoaBsfuqtZA"},"source":["## Task 4) Evaluate the best ALS model\n","\n","Take the trained cvModel and **extract the best parameter values** by inspecting the estimatorParameterMap. **Compare** the RMSE value of the trained model for different parameter settings to that of the mean.\n","\n","The parameter maps and metrics lists we get from the `cvModel` are local Python list, so we need to use standard Python methods, not RDD methods. There are however similar functions available, in particular `map` and `zip`, which work like for RDDs and `list` which is similar to RDD.collect in creating a mapped list. See here for documentation:  [https://docs.python.org/3/library/functions.html](https://docs.python.org/3/library/functions.html)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sU52b1iJqtZB"},"outputs":[],"source":["print('parameter combinations: ', cvModel.getEstimatorParamMaps()) # get parameter combinations\n","print('metrics: ', cvModel.avgMetrics) # the metrics from the CrossValidation\n","# use Python zip and list (not RDD functions, these are local Python objects)\n","# to create a joint parameter and result list\n",">>>paramMap = ... #<<<\n","print(paramMap) # for testing\n","print('\\n') # for readability of the output\n","# use Python min to get the best params (i.e. those producing minimal RMSE)\n","paramMax = min(paramMap, key=lambda x: x[1])\n","print('optimal parameters (giving lowest RMSE): ', paramMax)\n","# now we have the best parameters and the best test value from CV"]},{"cell_type":"markdown","metadata":{"id":"kiwbNbqkV_wr"},"source":["**Evaluate** the `cvModel` by computing the RMSE on the held-out test data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QNZ-Tw_ZV1m2"},"outputs":[],"source":[">>>test_pred = ... #<<< transform with the cvModel\n","rmse = regEval.evaluate(test_pred)\n","print(\"Root-mean-square error = \" + str(rmse))"]},{"cell_type":"markdown","metadata":{"id":"TBP8oxIkqtZE"},"source":["## Task 5) Scale up to Google Cloud\n","\n","**Apply** the approach above to the **larger MovieLens dataset** (or part of it). The data is available at `Big_Data/data/movielens-small/ratings.csv`.\n","It is available in CSV form, so that you will need to **adapt the code** to read that data.\n","\n","**Extract a Python file** from the code in this notebook, and apply that in the cloud (you can use the [`sparkSubmitDemo.py` example on Moodle](https://moodle4.city.ac.uk/mod/resource/view.php?id=381975) as a starting point). To upload the data to Google Storage, you can follow the [tutorial on Moodle](https://moodle4.city.ac.uk/pluginfile.php/441245/mod_resource/content/1/Google%20Cloud%20Intro%202024.pdf)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Milzo864LjL3"},"outputs":[],"source":[]}],"metadata":{"colab":{"private_outputs":true,"provenance":[{"file_id":"1lrKhaZLjpP7XijyLd6ZmI-pnR7D8-nTD","timestamp":1584059710072},{"file_id":"1eGiTCA_g4LlTsg5ZJw4r2ecN0eGyU6BN","timestamp":1582916481655}]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":0}