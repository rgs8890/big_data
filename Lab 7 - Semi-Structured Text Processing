{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1cOuVUXolQBpkS8lSV-DVXG_VZ7jU8Ryt","timestamp":1582202145308}]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"HkwMCDA2hNbs"},"source":["# Lab Sheet 7:  Semi-Structured Text Processing\n","\n","These tasks are for working in the lab session and during the week.  \n","\n","We will do some **XML parsing** and look into **web scraping**.\n"]},{"cell_type":"markdown","source":["First, we do the usual preliminaries."],"metadata":{"id":"gDuX-OB9xtyN"}},{"cell_type":"code","metadata":{"id":"RelUnfEOhNby","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1710520089644,"user_tz":0,"elapsed":70537,"user":{"displayName":"Tillman Weyde","userId":"17945142824947980120"}},"outputId":"5d1ab95c-a0bf-4bd8-f252-c7797c7086d3"},"source":["# Load the Drive helper and mount\n","from google.colab import drive\n","\n","# This will prompt for authorization.\n","drive.mount('/content/drive')\n","\n","#install spark\n","%cd\n","!apt-get update -qq\n","!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n","!tar -xzf \"/content/drive/My Drive/Big_Data/data/spark/spark-3.5.0-bin-hadoop3.tgz\"\n","!pip install -q findspark\n","\n","import os\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n","os.environ[\"SPARK_HOME\"] = \"/root/spark-3.5.0-bin-hadoop3\"\n","%cd /content\n","import findspark\n","findspark.init()\n","\n","import pyspark\n","# get a spark context\n","sc = pyspark.SparkContext.getOrCreate()\n","print(sc)\n","# and a spark session\n","spark = pyspark.sql.SparkSession.builder.getOrCreate()\n","print(spark)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/root\n","/content\n","<SparkContext master=local[*] appName=pyspark-shell>\n","<pyspark.sql.session.SparkSession object at 0x7a8210259ed0>\n"]}]},{"cell_type":"code","metadata":{"id":"QsXh6-ZFnp3L","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1710520089645,"user_tz":0,"elapsed":21,"user":{"displayName":"Tillman Weyde","userId":"17945142824947980120"}},"outputId":"f3bca4f8-50f6-4ab0-877b-752ae966211b"},"source":["%ls \"/content/drive/MyDrive/Big_Data/data\""],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[0m\u001b[01;34m20_newsgroups\u001b[0m/  hamlet.txt  \u001b[01;34mlingspam_public\u001b[0m/  \u001b[01;34mspark\u001b[0m/\n","\u001b[01;34mfoodhygiene\u001b[0m/    \u001b[01;34mlibrary\u001b[0m/    \u001b[01;34mmovielens-small\u001b[0m/  stopwords2.txt\n"]}]},{"cell_type":"markdown","metadata":{"id":"pL5ZgfAGhNc6"},"source":["\n","\n","## Analysing XML Data\n","\n","These tasks are for working in the lab session and during the week. These tasks use the skills you have learned so far on a new XML dataset.\n","\n","To get **different datasets**, use the link below and once you get the code working for one dataset, you can then try and experiment with the remaing sets.\n","\n","http://ratings.food.gov.uk/open-data/\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ERPfG2bthNc7"},"source":["We start by reading the dataset into memory and parsing it using the ElementTree parser.\n","The XML looks like this:\n","\n","```\n","<FHRSEstablishment>\n","    <Header>\n","    <ExtractDate>2015-11-06</ExtractDate>\n","    <ItemCount>1256</ItemCount>\n","    <ReturnCode>Success</ReturnCode>\n","    </Header>\n","    <EstablishmentCollection>\n","        <EstablishmentDetail>\n","        <FHRSID>507136</FHRSID>\n","        <LocalAuthorityBusinessID>PI/000081182</LocalAuthorityBusinessID>\n","        <BusinessName>196</BusinessName>\n","        <BusinessType>Restaurant/Cafe/Canteen</BusinessType>\n","        <BusinessTypeID>1</BusinessTypeID>\n","        <AddressLine1>Cambridge</AddressLine1>\n","        <AddressLine2>Cambridgeshire</AddressLine2>\n","        <PostCode>CB1 3NF</PostCode>\n","        <RatingValue>5</RatingValue>\n","        <RatingKey>fhrs_5_en-GB</RatingKey>\n","        <RatingDate>2015-01-22</RatingDate>\n","        <LocalAuthorityCode>027</LocalAuthorityCode>\n","        <LocalAuthorityName>Cambridge City</LocalAuthorityName>\n","            <LocalAuthorityWebSite>http://www.cambridge.gov.uk</LocalAuthorityWebSite>\n","            <LocalAuthorityEmailAddress>env.health@cambridge.gov.uk</LocalAuthorityEmailAddress>\n","        <Scores>\n","            <Hygiene>5</Hygiene>\n","            <Structural>0</Structural>\n","            <ConfidenceInManagement>5</ConfidenceInManagement>\n","        </Scores>\n","        <SchemeType>FHRS</SchemeType>\n","        <NewRatingPending>False</NewRatingPending>\n","        <Geocode>\n","            <Longitude>0.14503300000000</Longitude>\n","            <Latitude>52.19734500000000</Latitude>\n","        </Geocode>\n","    </EstablishmentDetail>\n","```\n","    "]},{"cell_type":"markdown","metadata":{"id":"qvcdM446hNc9"},"source":["## 1) Get the XML Data and parse it\n","\n","The predefined `parseXML` function creates an XML parse tree for you. Start creating an **RDD** that contains just the **parse trees** instead of text. Then **filter out** the files that **could not be parsed**.\n","\n","We provide the code for using ElementTree, but it's worth having a look for more info here: [https://docs.python.org/3/library/xml.etree.elementtree.html](https://docs.python.org/3/library/xml.etree.elementtree.html).\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"Y-u8Nov2hNdG","colab":{"base_uri":"https://localhost:8080/","height":105},"executionInfo":{"status":"error","timestamp":1710520089999,"user_tz":0,"elapsed":363,"user":{"displayName":"Tillman Weyde","userId":"17945142824947980120"}},"outputId":"748e7bd1-795b-4603-9560-f6aa4123e97e"},"source":["import xml.etree.ElementTree as ET\n","\n","# this function generates the parsetree\n","def parseXML(f_x):\n","    try:\n","        root = ET.fromstring(f_x[1])\n","    except ET.ParseError as err:\n","        # parsing error :-(\n","        root = None\n","    return (f_x[0], root)\n","\n","p = \"/content/drive/MyDrive/Big_Data/data/foodhygiene\"\n","rawData = sc.wholeTextFiles(p)\n",">>>parsedDataAll = rawData.map(lambda f_doc: ...) #<<<< map to XML parse trees\n",">>>parsedData = parsedDataAll.filter(lambda f_tree: ...) #<<< filter out items where the parse tree is \"None\"\n","print(parsedData.take(1))"],"execution_count":null,"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"invalid syntax (<ipython-input-3-3b555a3eb83f>, line 14)","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-3b555a3eb83f>\"\u001b[0;36m, line \u001b[0;32m14\u001b[0m\n\u001b[0;31m    >>>parsedDataAll = rawData.map(lambda f_doc: ...) #<<<< map to XML parse trees\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"]}]},{"cell_type":"markdown","metadata":{"id":"OiZvafXlhNdI"},"source":["## 2) Find establishements with valid hygiene ratings\n","\n","Find **all possible** RatingValue elements. For that we use the ElementTree function **`findall`**. We use a syntax called XPath, which enables us to find elements lower down in the tree without explicitly traversing it. The XPath syntax ist shown here: [https://docs.python.org/3/library/xml.etree.elementtree.html#supported-xpath-syntax](https://docs.python.org/3/library/xml.etree.elementtree.html#supported-xpath-syntax)\n","\n","For finding the rating values, use `element.findall('.//RatingValue')` where element should be the root of a parse tree. This gives a list of XML elements `x`. We can list their `x.text` properties, using a list comprehension."]},{"cell_type":"code","metadata":{"id":"NW5sPY7-hNdJ"},"source":[">>>allRatings = parsedData.flatMap(lambda f_tree: [... for x in ...]).distinct() #<<< find distinct values\n","allRatings.collect()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ENcvytRjhNdL"},"source":["## 3) Remove non-numeric ratings\n","\n","Now we want to **get rid of** the non-numeric ratings. These should be:\n","\n","    invalidRatings = ['Pass', 'Pass and Eat Safe', 'Awaiting Inspection', 'AwaitingInspection', 'Awaiting Publication', 'AwaitingPublication', 'Improvement Required', 'Exempt']\n","    \n","    \n","First collect all XML elements tagged `EstablishmentDetail` (with `findall` as before), but keep the elements for further use (i.e. don't extract the text element as above).\n","\n","Then filter the EstablishmentDetail RDD by comparing their text against the `invalidRatings`.\n"]},{"cell_type":"code","metadata":{"id":"ClgbdAUmhNdL"},"source":["invalidRatings = ['Pass', 'Pass and Eat Safe', 'Awaiting Inspection', 'AwaitingInspection', 'Awaiting Publication', 'AwaitingPublication', 'Improvement Required', 'Exempt']\n","\n",">>>allEstData = parsedData.flatMap(lambda f_tree: ...) #<<< get the establishment detail\n","print(allEstData.count()) # should be 536266\n","\n","estData = allEstData.filter(lambda est: est.find('RatingValue').text not in invalidRatings)\n","print(estData.count()) # should be 439259"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"of76PCQHhNdN"},"source":["## 4) Find highest and lowest values\n","\n","Find the **10 local authorities** with the **highest and lowest mean hygiene rating**. We now use `find` as there is only one local authority per establishment, which we use as key. We then get the RatingValue and **average** (not in **reduce**, so **associativity** is not required here).\n","\n","In order to take the top 10, we first **sort** the resulting RDD using the [`sortBy`](\n","https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.sortBy.html) method. It takes a function that specifies **which values to sort by**, so you can either create a lambda function selecting the part of the tuple you want to use as sort key, or have a look at the [`itemgetter`](https://docs.python.org/3/library/operator.html#operator.itemgetter) function in the [`operator`](https://docs.python.org/3/library/operator.html) standard library module, which provides such functionality for you. The `sortBy` method also has an optional keyword argument `ascending` by which you can control the sort order."]},{"cell_type":"code","metadata":{"id":"_s_Jhv3-hNdO"},"source":["from operator import add\n","\n","import numpy as np\n","authRatings = estData.map(lambda est: (est.find('LocalAuthorityName').text, [int(est.find('RatingValue').text)])) \\\n","    .reduceByKey(add) \\\n","    .map(lambda a_rl: (a_rl[0], np.mean(a_rl[1])))\n","\n",">>>print(authRatings.sortBy(...).take(10)) # using RDD.sortBy() get the 10 highest\n",">>>print(authRatings.sortBy(...).take(10)) # using RDD.sortBy() get the 10 lowest"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"k2g8rZXuhNdQ"},"source":["## 5) Organise by PostCode\n","Use the **first part** of the PostCode node (i.e. for IP7 5BY, only use IP7) to find the **best and worse postcodes** for food hygiene.\n"]},{"cell_type":"code","metadata":{"id":"jh_BcqxvhNdQ"},"source":["import re\n","\n","pc_pattern = re.compile(r'([A-Z][A-Z]?[0-9][A-Z0-9]?) *[0-9][A-Z]{2}')\n","\n","def parse_postcode(pc_string):\n","    pc_match = pc_pattern.match(pc_string)\n","    if pc_match:\n","        return pc_match.group(1)\n","    else:\n","        return ''\n","\n","authRatings = estData.map(lambda est:\n","    # organize by PostCode instead of LocalAuthority\n","    (parse_postcode('' if est.find('PostCode') is None else est.find('PostCode').text.upper()),\n","    [int(est.find('RatingValue').text)])) \\\n","    .reduceByKey(add) \\\n","    .map(lambda a_rl: (a_rl[0], np.mean(a_rl[1])))\n","# output as above\n",">>>print(authRatings.sortBy(...).take(10)) # using RDD.sortBy() get the 10 highest\n",">>>print(authRatings.sortBy(...).take(10)) # using RDD.sortBy() get the 10 lowest"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"32uc8Sm5hNdS"},"source":["## 6) Organise by Type\n","\n","Use the BusinessType or BusinessTypeID nodes to discover and consolidate all business types. Find the **10 best and worse rated business types** for the entire UK, per local authority and per postcode using your consolidated categories"]},{"cell_type":"code","metadata":{"id":"GCoveRN_hNdS"},"source":["authRatings = estData.map(lambda est:\n","    # organize by BusinessType instead of LocalAuthority\n","    (0 if est.find('BusinessType') is None else est.find('BusinessType').text,\n","    [int(est.find('RatingValue').text)])) \\\n","    .reduceByKey(add) \\\n","    .map(lambda a_rl: (a_rl[0], np.mean(a_rl[1])))\n","# ouput as above\n",">>>\n",">>>"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XH0hTm5nhNdU"},"source":["## 7) Extra info: Web Scraping with an HTML parser\n","\n","This is the scraper that was used to get the food hygiene data. This uses **some programming techniques that we haven't covered**. However, if you have some time, this is an **interesting topic** to look into, as the web is a boundless source of data."]},{"cell_type":"code","metadata":{"id":"gZtVJ2yvhNdU"},"source":["# This is a code to scrape data from the web page. You can use this for future use\n","# You do not have to run this today\n","# This will take a *long* time\n","\n","import urllib.request\n","from html.parser import HTMLParser\n","\n","%cd \"/content/drive/My Drive\"\n","\n","!mkdir webscrape\n","%cd webscrape\n","\n","class MyHTMLParser(HTMLParser):\n","    links = None\n","    def handle_starttag(self, tag, attrs):\n","        if self.links is None:\n","            self.links = []\n","        if tag == 'a':\n","            href = None\n","            for k,v in attrs: # keys and values of the tag attributes\n","                if k == 'href': # if the key is 'href'\n","                    href = v # we are interested in its value\n","            if href is not None: # if there is a link\n","                if href.endswith('en-GB.xml'): # and it is the type that we expect\n","                    self.links.append(href) # then add to our list of links\n","\n","base_url = \"http://ratings.food.gov.uk\"\n","\n","f = urllib.request.urlopen(base_url + \"/open-data\") # the ratings site\n","\n","parser = MyHTMLParser() # initiate our parser\n","parser.feed(str(f.read())) # read from the URL\n","for l in parser.links: # get the collected links\n","    fname = l.split('/') # split them\n","    fname = fname[-1]    # get filename\n","    print('downloading {}'.format(fname)) # print a message\n","    urllib.request.urlretrieve(base_url + l, fname)"],"execution_count":null,"outputs":[]}]}