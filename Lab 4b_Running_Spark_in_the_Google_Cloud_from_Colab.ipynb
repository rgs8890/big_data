{"cells":[{"cell_type":"markdown","metadata":{"id":"89B27-TGiDNB"},"source":["# Lab 4b: Connecting to the Google Cloud from Colab\n","\n","We will here introduce the **set-up** to access and use **Google Cloud from your notebook in Colab**.  \n","\n","In addition to authenticating with Google Drive, you'll need to **authenticate with the Google Cloud** system.\n","\n","Then you can use the command line tools `gsutil` (for storage) and `gcloud` (for other cloud tasks).\n","These are part of the gcloud CLI (command line interface), which is already installed in Colab.\n","If you want to work on a local machine, you will need to install them locally (see [https://cloud.google.com/sdk/docs/install](https://cloud.google.com/sdk/docs/install)).  \n","\n","There are detailed references available for\n","\n","gcloud: [https://cloud.google.com/sdk/gcloud/reference](https://cloud.google.com/sdk/gcloud/reference)\n","\n","gsutil [https://cloud.google.com/storage/docs/gsutil](https://cloud.google.com/storage/docs/gsutil)."]},{"cell_type":"markdown","metadata":{"id":"MSDlLsAZh_se"},"source":["### Cloud and Drive authentication\n","\n","This is for **authenticating with with Google Drive and Google Cloud**, so that we can create and use our own buckets and access Dataproc and AI-Platform.\n","\n","First, we mount Google Drive for persistent local storage."]},{"cell_type":"code","execution_count":1,"metadata":{"id":"HueTPFr-ZH-P","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1714222914526,"user_tz":-60,"elapsed":22615,"user":{"displayName":"Tillman Weyde","userId":"17945142824947980120"}},"outputId":"8de95c97-9f89-4561-ba6b-401e1cc3e3e6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounting google drive...\n","Mounted at /content/drive\n","/content/drive/MyDrive\n"," \u001b[0m\u001b[01;36maidetect_share\u001b[0m@\n","'Andalusien 2013.gmap'\n","'base (1).cpython-37.pyc'\n","'base (2).cpython-37.pyc'\n","'base (3).cpython-37.pyc'\n"," base.cpython-37.pyc\n"," \u001b[01;34mBD\u001b[0m/\n"," \u001b[01;34mBD-CW\u001b[0m/\n"," \u001b[01;36mBig_Data\u001b[0m@\n"," \u001b[01;34mBig_Data_Material\u001b[0m/\n","'Brazil 2013.gmap'\n"," CaseForSupport-Draft-v2.gdoc\n","\u001b[01;34m'Colab Notebooks'\u001b[0m/\n","'ConstantinosChristodoulidesReport (1).gdoc'\n"," ConstantinosChristodoulidesReport.gdoc\n","'Copy 2 of Form_SemanticMedia_FundingMiniProjects_DarwinMusicWeb.gdoc'\n","'Copy of Form_SemanticMedia_FundingMiniProjects_DarwinMusicWeb_4_26_backup.gdoc'\n"," \u001b[01;36mdatasets\u001b[0m@\n"," \u001b[01;34mDSP_AP\u001b[0m/\n"," EDA_wrangling_WALS.ipynb\n"," EMR_Velarde-Weyde_Meredith_GV6_DM1_TW.docx.gdoc\n"," \u001b[01;34mEric\u001b[0m/\n"," \u001b[01;34mESG\u001b[0m/\n"," \u001b[01;36mExtracting_relevant_documents\u001b[0m@\n","\u001b[01;36m'Fei - Kew Gardens - Project'\u001b[0m@\n","'Gmx Mail.gdoc'\n","\u001b[01;34m'Happy AI'\u001b[0m/\n"," hello-world.py\n"," history.pkl\n","'Identity rule models.gdoc'\n"," Imputed_scaled_FS_BLOOM_V2.csv.zip\n"," level1212_WALS.csv.zip\n","\"Making Good on LSTMs' Unfulfilled Promise.gdoc\"\n","'mask (1).cpython-37.pyc'\n","'mask (2).cpython-37.pyc'\n","'mask (3).cpython-37.pyc'\n"," mask.cpython-37.pyc\n","'Metafish Architecture.gdraw'\n"," \u001b[01;34mMIReS_PresentationFinalReviewMeeting\u001b[0m/\n","\u001b[01;34m'MSc Projects'\u001b[0m/\n"," \u001b[01;36mMy-City-Big-Data\u001b[0m@\n","'My Saved Places.gmap'\n","'Nadine PhD Weekly.gdoc'\n","'Nicholas Bailey PhD Spread Sheet.gsheet'\n"," \u001b[01;34mNLP-proj\u001b[0m/\n"," Oxford_Workshop_Big_Data_TW3.pptx.pdf\n","\u001b[01;34m'PhD projects'\u001b[0m/\n"," \u001b[01;36mPHDResults\u001b[0m@\n","\u001b[01;34m'PHDResults (1)'\u001b[0m/\n","'PhD summary.gdoc'\n","'Points to address in the revision of TIST paper.gdoc'\n"," \u001b[01;34mPrivate\u001b[0m/\n","'Pruned NN Approx .gdoc'\n","\u001b[01;34m'Research Proposals'\u001b[0m/\n"," \u001b[01;34mRR\u001b[0m/\n","'Semantic Media Mini-Project Ideas.gdoc'\n","'Seth’s project.gdoc'\n"," ShapExplainer.py\n","'sparse_global (1).cpython-37.pyc'\n"," sparse_global.cpython-37.pyc\n","'Tante Freda.vcf'\n","\u001b[01;36m'Teaching GenAI '\u001b[0m@\n"," \u001b[01;34mtest\u001b[0m/\n","'Topic Derivative-based learning (mbo-pgur-ioz – 21 Mar 2024).gjam'\n"," \u001b[01;34mtrainer\u001b[0m/\n"," \u001b[01;34mUserWeighting\u001b[0m/\n"," Veni2018form-20180105.pdf\n"," \u001b[01;34mwebscrape\u001b[0m/\n"," WeightRegRF.py\n","'Why ChatGPT is not coming for your job just yet.gdoc'\n"]}],"source":["print('Mounting google drive...')\n","from google.colab import drive\n","drive.mount('/content/drive')\n","%cd \"/content/drive/MyDrive\"\n","%ls"]},{"cell_type":"markdown","metadata":{"id":"l2zNNa2xxBcg"},"source":["Next, we authenticate with the cloud system to enable access to DataProc and AI-Platform."]},{"cell_type":"code","execution_count":2,"metadata":{"id":"nyt_ZnPIRe8K","executionInfo":{"status":"ok","timestamp":1714222929715,"user_tz":-60,"elapsed":15196,"user":{"displayName":"Tillman Weyde","userId":"17945142824947980120"}}},"outputs":[],"source":["import sys\n","if 'google.colab' in sys.modules:\n","    from google.colab import auth\n","    auth.authenticate_user()"]},{"cell_type":"markdown","metadata":{"id":"C5azF8xo3FNC"},"source":["Let's **create a new Google Cloud project** now.\n","Do this on the [GC Console page](https://console.cloud.google.com) by clicking on the entry at the top, right of the *Google Cloud Platform* and choosing *New Project*. **Copy** the **generated project ID** to the next cell. Also **enable billing** and the **Compute, Storage and Dataproc** APIs as explained in the Cloud Intro.\n","\n","We also specify the **default project and region**. The REGION should be `us-central1` as that seems to be the only one that reliably works with the free credit.\n","This way we don't have to specify this information every time we access the cloud."]},{"cell_type":"code","execution_count":3,"metadata":{"id":"d2GtQXvF2qlS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1714222936775,"user_tz":-60,"elapsed":7067,"user":{"displayName":"Tillman Weyde","userId":"17945142824947980120"}},"outputId":"aabcd4a0-91da-4c2f-ac08-7ecc42a7b564"},"outputs":[{"output_type":"stream","name":"stdout","text":["Updated property [core/project].\n","\u001b[1;33mWARNING:\u001b[0m Property validation for compute/region was skipped.\n","Updated property [compute/region].\n","Updated property [dataproc/region].\n","[component_manager]\n","disable_update_check = True\n","[compute]\n","region = us-central1\n","[core]\n","account = t.e.weyde@city.ac.uk\n","project = bd-labs-test\n","[dataproc]\n","region = us-central1\n","\n","Your active configuration is: [default]\n"]}],"source":["### this project NEEDS TO BE SET UP IN GOOGLE CLOUD FIRST\n","PROJECT = 'bd-labs-test' ### Append -xxxx, where xxxx is your City login to make project names unique ###\n","### it seems that the project name here has the be in lower case.\n","!gcloud config set project $PROJECT\n","REGION = 'us-central1' # this has worked most reliably with the free tier\n","!gcloud config set compute/region $REGION\n","!gcloud config set dataproc/region $REGION\n","\n","!gcloud config list # show some information"]},{"cell_type":"markdown","metadata":{"id":"TAl4A9PEhPnR"},"source":["With the cell below, we **create a storage bucket** that we will use later for **global storage**.\n","If the bucket exists you will see a \"ServiceException: 409 ...\", which does not cause any problems.\n","**You must create your own bucket to have write access.**"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"N7zaq0gThQRR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1714223043321,"user_tz":-60,"elapsed":10998,"user":{"displayName":"Tillman Weyde","userId":"17945142824947980120"}},"outputId":"d252409c-b3ea-4b38-f5ba-5860daa3ed89"},"outputs":[{"output_type":"stream","name":"stdout","text":["CommandException: 1 files/objects could not be removed.\n","Removing gs://bd-labs-test-storage2/...\n","Creating gs://bd-labs-test-storage2/...\n"]}],"source":["BUCKET = 'gs://{}-storage2'.format(PROJECT)\n","!gsutil mb -b on $BUCKET"]},{"cell_type":"markdown","metadata":{"id":"luldPUhppn2c"},"source":["### Running Spark in the cloud\n","\n","We will start by **to use Spark on GC Dataproc**.\n","\n","This section shows you in detail **how to run Python code in Dataproc**.\n","You may need to **enable the Dataproc API** on the [console Dataproc page](https://console.cloud.google.com/dataproc/clusters/), if you have not done so, yet."]},{"cell_type":"markdown","metadata":{"id":"ELL92lwEs7ju"},"source":["First we need to **create a cluster**. We start with a single machine, just to try it out.\n","\n","We are using the `gcloud dataproc clusters` command. [Click here for documentation](https://cloud.google.com/sdk/gcloud/reference/dataproc/clusters).\n","The **parameter** `--image-version 1.5-ubuntu18` makes sure we get **intended software**.\n","\n","Starting a cluster **can take a few minutes**. You can wait for the cell to finish processing or interrupt its execution and check on the [console Dataproc page](https://console.cloud.google.com/dataproc/clusters/) if the cluster is ready.\n"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"Pt9XM8yYtRjJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1714223385314,"user_tz":-60,"elapsed":325101,"user":{"displayName":"Tillman Weyde","userId":"17945142824947980120"}},"outputId":"fcae13c4-2713-46cf-e633-15bfbdeccb92"},"outputs":[{"output_type":"stream","name":"stdout","text":["Waiting on operation [projects/bd-labs-test/regions/us-central1/operations/35fe6d6a-1959-3cf5-b911-3763c7f5c6bd].\n","\n","\u001b[1;33mWARNING:\u001b[0m Creating clusters using the n1-standard-1 machine type is not recommended. Consider using a machine type with higher memory.\n","\u001b[1;33mWARNING:\u001b[0m The firewall rules for specified network or subnetwork would allow ingress traffic from 0.0.0.0/0, which could be a security risk.\n","\u001b[1;33mWARNING:\u001b[0m Unable to validate the staging bucket lifecycle configuration of the bucket 'dataproc-staging-us-central1-627592605246-f5boiwpi' due to an internal error, Please make sure that the provided bucket doesn't have any delete rules set.\n","Created [https://dataproc.googleapis.com/v1/projects/bd-labs-test/regions/us-central1/clusters/bd-labs-test-cluster] Cluster placed in zone [us-central1-c].\n"]}],"source":["CLUSTER = '{}-cluster'.format(PROJECT)\n","!gcloud dataproc clusters create $CLUSTER \\\n","    --image-version 1.5-ubuntu18 --single-node \\\n","    --master-machine-type n1-standard-4 \\\n","    --master-boot-disk-type pd-ssd --master-boot-disk-size 100 \\\n","    --max-idle 3600s"]},{"cell_type":"markdown","metadata":{"id":"Jjt47GKUWFHQ"},"source":["The `--max-idle 3600s` flag means that the cluster will be **deleted automatically** once it has been **idle for 1 hour**. This helps minimising costs for a cluster left running by accident.\n","\n","This is a single-node cluster. This is created a bit more quickly than a multi-node cluster, but set-up is still quite slow (several minutes) because of the **restrictions on the free tier**.\n","\n","If you stay **in the free tier Google promises not to charge you**. You could switch to **paid tier (not recommended)** and still use your free credit, but then you may have to **pay for usage**.\n","\n","The free tier resources are sufficient for us for now, if you use the local Spark installation for testing.\n","\n","We have not specified the region (we could have used `--region $REGION`) as we set already the default for Dataproc in the beginning.\n","\n","You can run the **command below to get extensive information** about your running cluster.\n","However, it is usually **more practical** to look at the [console Dataproc page](https://console.cloud.google.com/dataproc/clusters/).\n","\n","You can check the details and current state of your cluster by clicking on its name.\n","Double-check there at the end of your working session to make sure that no clusters are left running, especially when you are not in free mode any more."]},{"cell_type":"code","execution_count":9,"metadata":{"id":"jnymMq3MWnQX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1714223386823,"user_tz":-60,"elapsed":1520,"user":{"displayName":"Tillman Weyde","userId":"17945142824947980120"}},"outputId":"0f792e2f-d6bb-4101-8fb0-343cf34953af"},"outputs":[{"output_type":"stream","name":"stdout","text":["clusterName: bd-labs-test-cluster\n","clusterUuid: ef41a2f9-81d8-4baa-8595-902d7f532d15\n","config:\n","  configBucket: dataproc-staging-us-central1-627592605246-f5boiwpi\n","  endpointConfig: {}\n","  gceClusterConfig:\n","    internalIpOnly: false\n","    networkUri: https://www.googleapis.com/compute/v1/projects/bd-labs-test/global/networks/default\n","    serviceAccountScopes:\n","    - https://www.googleapis.com/auth/bigquery\n","    - https://www.googleapis.com/auth/bigtable.admin.table\n","    - https://www.googleapis.com/auth/bigtable.data\n","    - https://www.googleapis.com/auth/cloud.useraccounts.readonly\n","    - https://www.googleapis.com/auth/devstorage.full_control\n","    - https://www.googleapis.com/auth/devstorage.read_write\n","    - https://www.googleapis.com/auth/logging.write\n","    - https://www.googleapis.com/auth/monitoring.write\n","    zoneUri: https://www.googleapis.com/compute/v1/projects/bd-labs-test/zones/us-central1-c\n","  lifecycleConfig:\n","    idleDeleteTtl: 3600s\n","    idleStartTime: '2024-04-27T13:04:22.999058Z'\n","  masterConfig:\n","    diskConfig:\n","      bootDiskSizeGb: 100\n","      bootDiskType: pd-ssd\n","    imageUri: https://www.googleapis.com/compute/v1/projects/cloud-dataproc/global/images/dataproc-1-5-ubu18-20230909-165100-rc01\n","    instanceNames:\n","    - bd-labs-test-cluster-m\n","    machineTypeUri: https://www.googleapis.com/compute/v1/projects/bd-labs-test/zones/us-central1-c/machineTypes/n1-standard-1\n","    minCpuPlatform: AUTOMATIC\n","    numInstances: 1\n","    preemptibility: NON_PREEMPTIBLE\n","  softwareConfig:\n","    imageVersion: 1.5.90-ubuntu18\n","    properties:\n","      capacity-scheduler:yarn.scheduler.capacity.root.default.ordering-policy: fair\n","      core:fs.gs.block.size: '134217728'\n","      core:fs.gs.metadata.cache.enable: 'false'\n","      core:hadoop.ssl.enabled.protocols: TLSv1,TLSv1.1,TLSv1.2\n","      dataproc:dataproc.allow.zero.workers: 'true'\n","      distcp:mapreduce.map.java.opts: -Xmx576m\n","      distcp:mapreduce.map.memory.mb: '768'\n","      distcp:mapreduce.reduce.java.opts: -Xmx576m\n","      distcp:mapreduce.reduce.memory.mb: '768'\n","      hdfs:dfs.datanode.address: 0.0.0.0:9866\n","      hdfs:dfs.datanode.http.address: 0.0.0.0:9864\n","      hdfs:dfs.datanode.https.address: 0.0.0.0:9865\n","      hdfs:dfs.datanode.ipc.address: 0.0.0.0:9867\n","      hdfs:dfs.namenode.handler.count: '20'\n","      hdfs:dfs.namenode.http-address: 0.0.0.0:9870\n","      hdfs:dfs.namenode.https-address: 0.0.0.0:9871\n","      hdfs:dfs.namenode.lifeline.rpc-address: bd-labs-test-cluster-m:8050\n","      hdfs:dfs.namenode.secondary.http-address: 0.0.0.0:9868\n","      hdfs:dfs.namenode.secondary.https-address: 0.0.0.0:9869\n","      hdfs:dfs.namenode.service.handler.count: '10'\n","      hdfs:dfs.namenode.servicerpc-address: bd-labs-test-cluster-m:8051\n","      hive:hive.fetch.task.conversion: none\n","      mapred-env:HADOOP_JOB_HISTORYSERVER_HEAPSIZE: '1000'\n","      mapred:mapreduce.job.maps: '6'\n","      mapred:mapreduce.job.reduce.slowstart.completedmaps: '0.95'\n","      mapred:mapreduce.job.reduces: '1'\n","      mapred:mapreduce.jobhistory.recovery.store.class: org.apache.hadoop.mapreduce.v2.hs.HistoryServerLeveldbStateStoreService\n","      mapred:mapreduce.map.cpu.vcores: '1'\n","      mapred:mapreduce.map.java.opts: -Xmx819m\n","      mapred:mapreduce.map.memory.mb: '1024'\n","      mapred:mapreduce.reduce.cpu.vcores: '1'\n","      mapred:mapreduce.reduce.java.opts: -Xmx1638m\n","      mapred:mapreduce.reduce.memory.mb: '2048'\n","      mapred:mapreduce.task.io.sort.mb: '256'\n","      mapred:yarn.app.mapreduce.am.command-opts: -Xmx819m\n","      mapred:yarn.app.mapreduce.am.resource.cpu-vcores: '1'\n","      mapred:yarn.app.mapreduce.am.resource.mb: '1024'\n","      spark-env:SPARK_DAEMON_MEMORY: 1000m\n","      spark:spark.driver.maxResultSize: 480m\n","      spark:spark.driver.memory: 960m\n","      spark:spark.executor.cores: '1'\n","      spark:spark.executor.instances: '2'\n","      spark:spark.executor.memory: 1664m\n","      spark:spark.executorEnv.OPENBLAS_NUM_THREADS: '1'\n","      spark:spark.extraListeners: com.google.cloud.spark.performance.DataprocMetricsListener\n","      spark:spark.scheduler.mode: FAIR\n","      spark:spark.sql.cbo.enabled: 'true'\n","      spark:spark.ui.port: '0'\n","      spark:spark.yarn.am.memory: 640m\n","      yarn-env:YARN_NODEMANAGER_HEAPSIZE: '1000'\n","      yarn-env:YARN_RESOURCEMANAGER_HEAPSIZE: '1000'\n","      yarn-env:YARN_TIMELINESERVER_HEAPSIZE: '1000'\n","      yarn:yarn.nodemanager.address: 0.0.0.0:8026\n","      yarn:yarn.nodemanager.resource.cpu-vcores: '1'\n","      yarn:yarn.nodemanager.resource.memory-mb: '3072'\n","      yarn:yarn.resourcemanager.nodemanager-graceful-decommission-timeout-secs: '86400'\n","      yarn:yarn.scheduler.maximum-allocation-mb: '3072'\n","      yarn:yarn.scheduler.minimum-allocation-mb: '256'\n","  tempBucket: dataproc-temp-us-central1-627592605246-okemnct7\n","labels:\n","  goog-dataproc-autozone: enabled\n","  goog-dataproc-cluster-name: bd-labs-test-cluster\n","  goog-dataproc-cluster-uuid: ef41a2f9-81d8-4baa-8595-902d7f532d15\n","  goog-dataproc-location: us-central1\n","projectId: bd-labs-test\n","status:\n","  state: RUNNING\n","  stateStartTime: '2024-04-27T13:09:39.297766Z'\n","statusHistory:\n","- state: CREATING\n","  stateStartTime: '2024-04-27T13:04:22.999058Z'\n"]}],"source":["!gcloud dataproc clusters describe $CLUSTER"]},{"cell_type":"markdown","metadata":{"id":"Y-A4ZVjC2drX"},"source":["Now that our cluster is running, we can submit a Spark job. A minimal Spark job is just a Python script. A simple \"Hello World\" Spark script is provided in a public cloud bucket. Let's have a look at it:"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"ITTVHjWWXQsL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1714223389087,"user_tz":-60,"elapsed":2269,"user":{"displayName":"Tillman Weyde","userId":"17945142824947980120"}},"outputId":"0f9283a5-cbf2-448a-c6ee-9896e94a22c7"},"outputs":[{"output_type":"stream","name":"stdout","text":["#!/usr/bin/python\n","import pyspark\n","sc = pyspark.SparkContext()\n","rdd = sc.parallelize(['Hello,', 'world!'])\n","words = sorted(rdd.collect())\n","print(words)\n","\n"]}],"source":["!gsutil cat gs://dataproc-examples/pyspark/hello-world/hello-world.py"]},{"cell_type":"markdown","metadata":{"id":"t2Yz0RxVXSzD"},"source":["... and run it on the cluster. We submit the job with the `gcloud dataproc jobs` command ([click here for the documentation](https://cloud.google.com/sdk/gcloud/reference/dataproc/jobs)) with the cluster name.\n"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"9H9IsWYMVlDq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1714223454978,"user_tz":-60,"elapsed":65906,"user":{"displayName":"Tillman Weyde","userId":"17945142824947980120"}},"outputId":"a4b0ce43-144f-4b0e-b126-d233d5104d4b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Job [efe4422c4a4b47028512b57f99c61a06] submitted.\n","Waiting for job output...\n","24/04/27 13:10:01 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker\n","24/04/27 13:10:01 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster\n","24/04/27 13:10:01 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator\n","24/04/27 13:10:01 INFO org.spark_project.jetty.util.log: Logging initialized @6777ms to org.spark_project.jetty.util.log.Slf4jLog\n","24/04/27 13:10:01 INFO org.spark_project.jetty.server.Server: jetty-9.4.z-SNAPSHOT; built: unknown; git: unknown; jvm 1.8.0_382-b05\n","24/04/27 13:10:01 INFO org.spark_project.jetty.server.Server: Started @7033ms\n","24/04/27 13:10:01 INFO org.spark_project.jetty.server.AbstractConnector: Started ServerConnector@7d7e2198{HTTP/1.1, (http/1.1)}{0.0.0.0:32839}\n","24/04/27 13:10:04 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at bd-labs-test-cluster-m/10.128.0.24:8032\n","24/04/27 13:10:04 INFO org.apache.hadoop.yarn.client.AHSProxy: Connecting to Application History server at bd-labs-test-cluster-m/10.128.0.24:10200\n","24/04/27 13:10:05 INFO org.apache.hadoop.conf.Configuration: resource-types.xml not found\n","24/04/27 13:10:05 INFO org.apache.hadoop.yarn.util.resource.ResourceUtils: Unable to find 'resource-types.xml'.\n","24/04/27 13:10:05 INFO org.apache.hadoop.yarn.util.resource.ResourceUtils: Adding resource type - name = memory-mb, units = Mi, type = COUNTABLE\n","24/04/27 13:10:05 INFO org.apache.hadoop.yarn.util.resource.ResourceUtils: Adding resource type - name = vcores, units = , type = COUNTABLE\n","24/04/27 13:10:10 INFO org.apache.hadoop.yarn.client.api.impl.YarnClientImpl: Submitted application application_1714223298469_0001\n","['Hello,', 'world!']\n","24/04/27 13:10:46 INFO org.spark_project.jetty.server.AbstractConnector: Stopped Spark@7d7e2198{HTTP/1.1, (http/1.1)}{0.0.0.0:0}\n","Job [efe4422c4a4b47028512b57f99c61a06] finished successfully.\n","done: true\n","driverControlFilesUri: gs://dataproc-staging-us-central1-627592605246-f5boiwpi/google-cloud-dataproc-metainfo/ef41a2f9-81d8-4baa-8595-902d7f532d15/jobs/efe4422c4a4b47028512b57f99c61a06/\n","driverOutputResourceUri: gs://dataproc-staging-us-central1-627592605246-f5boiwpi/google-cloud-dataproc-metainfo/ef41a2f9-81d8-4baa-8595-902d7f532d15/jobs/efe4422c4a4b47028512b57f99c61a06/driveroutput\n","jobUuid: ec54c259-6499-37a1-83a9-c718236ce0fd\n","placement:\n","  clusterName: bd-labs-test-cluster\n","  clusterUuid: ef41a2f9-81d8-4baa-8595-902d7f532d15\n","pysparkJob:\n","  mainPythonFileUri: gs://dataproc-examples/pyspark/hello-world/hello-world.py\n","reference:\n","  jobId: efe4422c4a4b47028512b57f99c61a06\n","  projectId: bd-labs-test\n","status:\n","  state: DONE\n","  stateStartTime: '2024-04-27T13:10:52.264463Z'\n","statusHistory:\n","- state: PENDING\n","  stateStartTime: '2024-04-27T13:09:50.076118Z'\n","- state: SETUP_DONE\n","  stateStartTime: '2024-04-27T13:09:52.125547Z'\n","- details: Agent reported job success\n","  state: RUNNING\n","  stateStartTime: '2024-04-27T13:09:52.565027Z'\n","yarnApplications:\n","- name: hello-world.py\n","  progress: 1.0\n","  state: FINISHED\n","  trackingUrl: http://bd-labs-test-cluster-m:8088/proxy/application_1714223298469_0001/\n"]}],"source":["!gcloud dataproc jobs submit pyspark --cluster $CLUSTER \\\n","    gs://dataproc-examples/pyspark/hello-world/hello-world.py"]},{"cell_type":"markdown","metadata":{"id":"YiNBAoQiXDe0"},"source":["The `trackingUrl` shown above will only work as long as the job is running. On to the [dataproc page](https://console.cloud.google.com/dataproc/clusters/), you can click through to your cluster page and from there to your job page to see details on past jobs.\n","\n","You may get some warnings like this:\n","`WARN org.apache.spark.scheduler.FairSchedulableBuilder: Fair' ...\n","or this one:\n","`WARN org.apache.hadoop.hdfs.DataStreamer: Caught exception`. There was not enough time to research these fully but they don't affect what we do here and can be ignored.\n","\n","One issue is that we need to **get output data from the cluster back to the notebook**. We can output text through printing into the output stream, but that does not work well for scaling up, for automation, or for binary data.\n","\n","A better solution is to **pass an argument** to the job, to tell the job on the cluster **where to write the output**.\n","This requires a bit of extra code as shown below using the `argparse` package.\n","The example below the checks if the script runs in Colab and if that is the case, it does not execute the run function.\n","This is useful for quick testing with a local spark instance.\n","\n","For running the file in DataProc, write to a local file `hello-world.py`, uncomment the  the `%%writefile` magic as in the cell below.\n"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"Hvcr5pQ8WCeJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1714223703121,"user_tz":-60,"elapsed":830,"user":{"displayName":"Tillman Weyde","userId":"17945142824947980120"}},"outputId":"249d0749-fba8-40a5-97d7-fceee42c5d0c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting hello-world.py\n"]}],"source":["%%writefile hello-world.py\n","\n","import sys\n","import pyspark\n","import argparse\n","import pickle\n","\n","def save(object,bucket,filename):\n","    with open(filename,mode='wb') as f:\n","        pickle.dump(object,f)\n","    print(\"Saving {} to {}\".format(filename,bucket))\n","    import subprocess\n","    proc = subprocess.run([\"gsutil\",\"cp\", filename, bucket],stderr=subprocess.PIPE)\n","    print(\"gstuil returned: \" + str(proc.returncode))\n","    print(str(proc.stderr))\n","\n","def runWordCount(argv):\n","    # Parse the provided arguments\n","    print(argv)\n","    parser = argparse.ArgumentParser() # get a parser object\n","    parser.add_argument('--out_bucket', metavar='out_bucket', required=True,\n","                        help='The bucket URL for the result.') # add a required argument\n","    parser.add_argument('--out_file', metavar='out_file', required=True,\n","                        help='The filename for the result.') # add a required argument\n","    args = parser.parse_args(argv) # read the value\n","    # the value provided with --out_bucket is now in args.out_bucket\n","    sc = pyspark.SparkContext.getOrCreate()\n","    rdd = sc.parallelize(['Hello,', 'world!'])\n","    words = sorted(rdd.collect())\n","    save(words,args.out_bucket,args.out_file)\n","\n","if  'google.colab' not in sys.modules: # Don't use system arguments running in Colab\n","    runWordCount(sys.argv[1:])\n","elif __name__ == \"__main__\" : # but define them manually\n","    runWordCount([\"--out_bucket\", BUCKET, \"--out_file\", \"words.pkl\"])"]},{"cell_type":"markdown","metadata":{"id":"rl72JssUGyoi"},"source":["**Once the code works as intended**, you can write it to the local disk (on your Colab instance). For this, **uncomment the first line with the `%%writefile` magic** and then **re-run the cell**.\n","\n","Then **check** that the file **is in the current directory** and **has the right contents** like this:"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"CUDTWpVpG3oZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1714223705905,"user_tz":-60,"elapsed":655,"user":{"displayName":"Tillman Weyde","userId":"17945142824947980120"}},"outputId":"092717f4-c3f4-4f36-a7d6-63650ef620e7"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive\n","-rw------- 1 root root 1402 Apr 27 13:15 hello-world.py\n","\n","import sys\n","import pyspark\n","import argparse\n","import pickle\n","\n","def save(object,bucket,filename):\n","    with open(filename,mode='wb') as f:\n","        pickle.dump(object,f)\n","    print(\"Saving {} to {}\".format(filename,bucket))\n","    import subprocess\n","    proc = subprocess.run([\"gsutil\",\"cp\", filename, bucket],stderr=subprocess.PIPE)\n","    print(\"gstuil returned: \" + str(proc.returncode))\n","    print(str(proc.stderr))\n","\n","def runWordCount(argv):\n","    # Parse the provided arguments\n","    print(argv)\n","    parser = argparse.ArgumentParser() # get a parser object\n","    parser.add_argument('--out_bucket', metavar='out_bucket', required=True,\n","                        help='The bucket URL for the result.') # add a required argument\n","    parser.add_argument('--out_file', metavar='out_file', required=True,\n","                        help='The filename for the result.') # add a required argument\n","    args = parser.parse_args(argv) # read the value\n","    # the value provided with --out_bucket is now in args.out_bucket\n","    sc = pyspark.SparkContext.getOrCreate()\n","    rdd = sc.parallelize(['Hello,', 'world!'])\n","    words = sorted(rdd.collect())\n","    save(words,args.out_bucket,args.out_file)\n","\n","if  'google.colab' not in sys.modules: # Don't use system arguments running in Colab\n","    runWordCount(sys.argv[1:])\n","elif __name__ == \"__main__\" : # but define them manually\n","    runWordCount([\"--out_bucket\", BUCKET, \"--out_file\", \"words.pkl\"])\n"]}],"source":["!pwd\n","!ls -l hello-world.py\n","!cat hello-world.py"]},{"cell_type":"markdown","metadata":{"id":"ZnS846qFC2B9"},"source":["We can now submit the job with an extra section for application arguments. It's started by `--`, which indicates that all following arguments are to be sent to our Spark application."]},{"cell_type":"code","execution_count":14,"metadata":{"id":"JoI9owUfuGGJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1714227488156,"user_tz":-60,"elapsed":3775585,"user":{"displayName":"Tillman Weyde","userId":"17945142824947980120"}},"outputId":"5eed601f-1a10-4adb-eeba-f6aaebd1baf2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Job [5cf15dd1d08341b298e61ebcd55e7a52] submitted.\n","Waiting for job output...\n","['--out_bucket', 'gs://bd-labs-test-storage2', '--out_file', 'words.pkl']\n","24/04/27 13:15:23 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker\n","24/04/27 13:15:23 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster\n","24/04/27 13:15:23 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator\n","24/04/27 13:15:23 INFO org.spark_project.jetty.util.log: Logging initialized @7710ms to org.spark_project.jetty.util.log.Slf4jLog\n","24/04/27 13:15:24 INFO org.spark_project.jetty.server.Server: jetty-9.4.z-SNAPSHOT; built: unknown; git: unknown; jvm 1.8.0_382-b05\n","24/04/27 13:15:24 INFO org.spark_project.jetty.server.Server: Started @8015ms\n","24/04/27 13:15:24 INFO org.spark_project.jetty.server.AbstractConnector: Started ServerConnector@44eee42{HTTP/1.1, (http/1.1)}{0.0.0.0:38027}\n","24/04/27 13:15:27 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at bd-labs-test-cluster-m/10.128.0.24:8032\n","24/04/27 13:15:27 INFO org.apache.hadoop.yarn.client.AHSProxy: Connecting to Application History server at bd-labs-test-cluster-m/10.128.0.24:10200\n","24/04/27 13:15:27 INFO org.apache.hadoop.conf.Configuration: resource-types.xml not found\n","24/04/27 13:15:27 INFO org.apache.hadoop.yarn.util.resource.ResourceUtils: Unable to find 'resource-types.xml'.\n","24/04/27 13:15:27 INFO org.apache.hadoop.yarn.util.resource.ResourceUtils: Adding resource type - name = memory-mb, units = Mi, type = COUNTABLE\n","24/04/27 13:15:27 INFO org.apache.hadoop.yarn.util.resource.ResourceUtils: Adding resource type - name = vcores, units = , type = COUNTABLE\n","24/04/27 13:15:31 INFO org.apache.hadoop.yarn.client.api.impl.YarnClientImpl: Submitted application application_1714223298469_0002\n","24/04/27 13:33:58 WARN org.apache.spark.HeartbeatReceiver: Removing executor 1 with no recent heartbeats: 377602 ms exceeds timeout 120000 ms\n","\u001b[1;31mERROR:\u001b[0m (gcloud.dataproc.jobs.submit.pyspark) Job [5cf15dd1d08341b298e61ebcd55e7a52] failed with error:\n","Google Cloud Dataproc Agent reports job failure. If logs are available, they can be found at:\n","https://console.cloud.google.com/dataproc/jobs/5cf15dd1d08341b298e61ebcd55e7a52?project=bd-labs-test&region=us-central1\n","gcloud dataproc jobs wait '5cf15dd1d08341b298e61ebcd55e7a52' --region 'us-central1' --project 'bd-labs-test'\n","https://console.cloud.google.com/storage/browser/dataproc-staging-us-central1-627592605246-f5boiwpi/google-cloud-dataproc-metainfo/ef41a2f9-81d8-4baa-8595-902d7f532d15/jobs/5cf15dd1d08341b298e61ebcd55e7a52/\n","gs://dataproc-staging-us-central1-627592605246-f5boiwpi/google-cloud-dataproc-metainfo/ef41a2f9-81d8-4baa-8595-902d7f532d15/jobs/5cf15dd1d08341b298e61ebcd55e7a52/driveroutput\n"]}],"source":["FILENAME = 'words.pkl'\n","!gcloud dataproc jobs submit pyspark --cluster $CLUSTER --region $REGION \\\n","    ./hello-world.py \\\n","    -- --out_bucket $BUCKET --out_file $FILENAME"]},{"cell_type":"markdown","metadata":{"id":"jPvy_iDTPZTo"},"source":["Once the job has finished, we can **use the output** by **copying it from the bucket** and **reading it as a local file**."]},{"cell_type":"code","execution_count":15,"metadata":{"id":"wD0jG0YiBV_l","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1714227491920,"user_tz":-60,"elapsed":3799,"user":{"displayName":"Tillman Weyde","userId":"17945142824947980120"}},"outputId":"3547b504-d760-435c-c347-5e79984557be"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/BD-CW\n","CommandException: No URLs matched: gs://bd-labs-test-storage2/words.pkl\n","total 42\n","drwx------ 2 root root 4096 Feb 20  2022 flowers_training87\n","drwx------ 2 root root 4096 Feb 20  2022 flowers_training89\n","-rw------- 1 root root 1431 Feb 20  2022 hello-world.py\n","-rw------- 1 root root  196 Apr 30  2022 history.pkl\n","-rw------- 1 root root 1360 Apr 22  2022 param_res-220422-1717.pkl\n","-rw------- 1 root root 4545 Feb 20  2022 spark_job2.py\n","-rw------- 1 root root 7445 Feb 20  2022 spark_job.py\n","-rw------- 1 root root 7427 Aug  7  2022 spark_speed_test.py\n","-rw------- 1 root root 4477 Apr 27 12:42 spark_write_tfrec.py\n","-rw------- 1 root root 1360 Apr 22  2022 test.pkl\n","-rw------- 1 root root   50 Apr 30  2022 time.pkl\n","drwx------ 2 root root 4096 Feb 20  2022 trainer\n","-rw------- 1 root root   34 Feb 20  2022 words.pkl\n","Content of words.pkl : ['Hello,', 'world!']\n"]}],"source":["# Make sure you are writing to the right directory\n","import pickle\n","%cd /content/drive/MyDrive/BD-CW\n","!gsutil cp $BUCKET/$FILENAME .\n","!ls -l\n","with open(FILENAME,mode='rb') as f:\n","    words = pickle.load(f)\n","\n","print(\"Content of {} : {}\".format(FILENAME,words))"]},{"cell_type":"markdown","metadata":{"id":"MXYVsztX2uyM"},"source":["At the end of a session we should **delete the cluster**, as it incurs a **cost for the time** it runs.  "]},{"cell_type":"code","execution_count":14,"metadata":{"id":"l6LMV7cOuTQK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1714172728766,"user_tz":-60,"elapsed":60737,"user":{"displayName":"Tillman Weyde","userId":"17945142824947980120"}},"outputId":"411c6013-683d-4909-8ce9-26a0d3edac76"},"outputs":[{"output_type":"stream","name":"stdout","text":["Waiting on operation [projects/bd-labs-test/regions/us-central1/operations/c1af5690-763e-30d8-aa4d-0fdcda863fd8].\n","Deleted [https://dataproc.googleapis.com/v1/projects/bd-labs-test/regions/us-central1/clusters/bd-labs-test-cluster].\n"]}],"source":["!gcloud dataproc clusters delete $CLUSTER -q\n","# the -q flag disables the confirmation prompt\n","# , we want to make sure it really gets deleted"]},{"cell_type":"markdown","metadata":{"id":"8Ts44bFzGw2a"},"source":["###  Set up python packages on a cluster\n","\n","\n","In order to run our code on the cluster we need to install packages.  \n","For this, we enable package installation by passing a flag `--initialization-actions` with argument `gs://goog-dataproc-initialization-actions-$REGION/python/pip-install.sh` (this is a public script that will read metadata to determine which packages to install).\n","The packages are then specified by providing a `--metadata` flag with the argument `PIP_PACKAGES=<name==version>`, e.g . `PIP_PACKAGES=tensorflow` if we want TensorFlow installed."]},{"cell_type":"markdown","metadata":{"id":"WuXtPhZi2Ghp"},"source":["Once the cluster is running, you can run the job. It is useful to create a new filename, so that results don't get overwritten.\n","\n","You can for instance use `str(datetime.datetime.now().strftime(\"%y%m%d-%H%M\"))` to get a string with the current date and time and use that in the file name.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Lg_zGdoLdC6w"},"outputs":[],"source":["import datetime\n","time_string = str(datetime.datetime.now().strftime(\"%y%m%d-%H%M\"))\n","time_string"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":0}