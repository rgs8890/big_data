{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1AVbECIUnJcjz-blM7XeFNeETJQSnwpyw","timestamp":1581609108024},{"file_id":"1oUu05Wpzxx1VaWFtdwC2NnSoM1lsdgyQ","timestamp":1580399768359}]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Xs3o00tAfXhD"},"source":["# Lab Sheet 3a: More Word Frequency Vectors with Spark\n","\n","These tasks are for working in the lab session and during the week.\n","\n","We will use the **same data as in week 2** (18 files in \"/content/drive/My Drive/Big_Data/data/library\") and use **some more RDD functions**. We will apply **two different approaches** to create and use **fixed size vectors**.\n"]},{"cell_type":"markdown","metadata":{"id":"9bzKmDsjQavN"},"source":["\n","First, run the usual preparations."]},{"cell_type":"code","metadata":{"id":"RVWRS7O6fXhG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1708024581552,"user_tz":0,"elapsed":33469,"user":{"displayName":"Tillman Weyde","userId":"17945142824947980120"}},"outputId":"089bf6fb-f68a-46a2-a24a-fdce73d143c9"},"source":["# Load the Drive helper and mount\n","from google.colab import drive\n","\n","# This will prompt for authorization.\n","drive.mount('/content/drive')\n","\n","# install spark\n","%cd\n","!apt-get install openjdk-8-jdk-headless -qq > /dev/null # installing java\n","!tar -xzf \"/content/drive/My Drive/Big_Data/data/spark/spark-3.5.0-bin-hadoop3.tgz\" # unpacking\n","import os # Python package for interaction with the operating system\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\" # tell the system where Java lives\n","os.environ[\"SPARK_HOME\"] = \"/root/spark-3.5.0-bin-hadoop3\" # and where spark lives\n","!pip install -q findspark # install helper package\n","import findspark # use the helper package\n","findspark.init() # to set up spark\n","%cd \"/content/drive/My Drive/Big_Data\"\n","\n","import pyspark\n","# get a spark context\n","sc = pyspark.SparkContext.getOrCreate()\n","print(sc)\n","# and a spark session\n","spark = pyspark.sql.SparkSession.builder.getOrCreate()\n","print(spark)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/root\n","/content/drive/MyDrive/Big_Data_Material/Big_Data\n","<SparkContext master=local[*] appName=pyspark-shell>\n","<pyspark.sql.session.SparkSession object at 0x7b073a1342e0>\n"]}]},{"cell_type":"code","source":["!ls /content/drive/My\\ Drive/Big_Data/data/spark"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-6k7MRSCMN9U","executionInfo":{"status":"ok","timestamp":1708024505311,"user_tz":0,"elapsed":262,"user":{"displayName":"Tillman Weyde","userId":"17945142824947980120"}},"outputId":"03838faf-2f2c-43e0-f9db-a2cbee5c3c5b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":[" data\t\t\t\t\t  'Lab 2 - Extracting Word Freq Vectors.ipynb'\n","'Lab 1 - Word Counting with Spark.ipynb'   solutions\n"]}]},{"cell_type":"markdown","metadata":{"id":"C4mqxc6JfXhP"},"source":["Here is **code from week 2** that we **run first**, and then extend."]},{"cell_type":"code","metadata":{"id":"0CPuhOgBfXhS"},"source":["import re\n","import os.path\n","\n","def stripFinalS( word ):\n","    word = word.lower() # lower case\n","    word = word.rstrip('s')\n","    return word\n","\n","def splitFileWords(filenameContent): # your splitting function\n","    f, c = filenameContent # unpack the input tuple\n","    fwLst = [] # the new list for (filename,word) tuples\n","    wLst = re.split('\\W+',c) # <<< now create a word list wLst by splitting c (the content)\n","    for w in wLst : # iterate through the list\n","        fwLst.append((f, stripFinalS(w))) # and append (f,w) to the fwLst\n","    return fwLst # return a list of (f,w) tuples\n","\n","from pyspark import SparkContext\n","\n","sc = SparkContext.getOrCreate()\n","\n","dirPath = \"/content/drive/My Drive/Big_Data/data/library\" #  path\n","ft_RDD = sc.wholeTextFiles(dirPath) # create an RDD with wholeTextFiles\n","# just take filenames, drop path and extension for readability\n","fnt_RDD = ft_RDD.map(lambda ft: (os.path.splitext(os.path.basename(ft[0]))[0], ft[1]))\n","fw_RDD1 = fnt_RDD.flatMap(splitFileWords) # split words per file, strip final 's'\n","fw_RDD = fw_RDD1.filter(lambda fw: fw[1] not in ['','project','gutenberg', 'ebook']) # get rid of some unwanted words\n","fw_RDD.take(3)\n","# output should look like this: [('emma', 'the'), ('emma', 'of'), ('emma', 'emma')]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"um_y11FFfXhZ"},"source":["## 1) Warm-up\n","Let's start with a **few small tasks**, to become more fluent with **RDDs and lambda expressions**.\n","\n","1. Count the **number of documents**.\n","2. Determine the number **distinct words** in total (the vocabulary size) using `RDD.distinct()`. This involves **removing the filenames** from the `(f, w)` pairs and getting the RDD size (with `RDD.count()`).\n","3. Get the number of **words** (including repeated ones) per book.\n","4. Determine the number of distinct words per book. This involves determining the distinct `(f,w)` pairs, getting a list of words per file, and getting the list size.\n","5. Count the average number of occurrences per word per file (words/vocabulary). Use `RDD.join()` to get both numbers into one RDD.\n","\n","Remember that `>>>` indicates a line where you should do something - you need to remove it for any code to work.\n","Typically, you'll find a `...` placeholder in that line at the place where you should add the code.  "]},{"cell_type":"markdown","metadata":{"id":"rP8grfiZTOq3"},"source":["### a) Library size"]},{"cell_type":"code","metadata":{"id":"M-6pYyX3TN0-"},"source":[">>>print(\"Number of documents: \", ft_RDD ...) # count the number of docs"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Jb9xIH0VTNow"},"source":["### b) Vocabulary size"]},{"cell_type":"code","metadata":{"id":"P-9_Vq4ATNcu"},"source":[">>>w_RDD = fw_RDD.map(...) # remove the file names, keep just the words\n",">>>w_RDDu = w_RDD ... # keep only one unique instance of every word\n","print('Total vocabulary size: ', w_RDDu.count())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mr1XrmSxTNPi"},"source":["### c) Words per book"]},{"cell_type":"code","metadata":{"id":"w9M9poTqTMB9"},"source":["from operator import add\n",">>>f1_RDD = fw_RDD.map(lambda fw: ...) # wrap (f, w) to (f, 1)\n",">>>fc_RDD = f1_RDD.reduceByKey(...) # add up the 1s\n","print('Words per book: ', fc_RDD.take(3))\n","# extra task: try also to express this with just one function that appeared in the lecture last week\n",">>> print('Words per book (v2): ', fw_RDD ...)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"W5Ys75vDTLFW"},"source":["### d) Vocabulary per book"]},{"cell_type":"code","metadata":{"id":"tT3oOYPMTQZv"},"source":["# we can reuse the same solution as above, if we do one thing before ...\n",">>> fw_RDDu = ...\n","f1_RDDu = fw_RDDu.map(lambda fw: (fw[0], 1)) # wrap (f, w) to (f, 1)\n","fcu_RDD = f1_RDDu.reduceByKey(add) # add up the 1s\n","print('Vocabulary per book: ', fcu_RDD.take(3))\n","# extra task: replacing the map and reduce by one function as in c)\n",">>> print('Vocabulary per book (v2): ', fw_RDDu ... )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5G6cCLOfTRLt"},"source":["### e) Average occurrences of words per book (i.e. words/vocab per book)"]},{"cell_type":"code","metadata":{"id":"m76EZmrXTRtt"},"source":[">>>f_wv_RDD = fc_RDD ... fcu_RDD # join the two RDDs to get (f, (w, v)) tuples\n","print(f_wv_RDD.take(3))\n",">>>f_awo_RDD = f_wv_RDD.map(lambda f_wv: (f_wv[0], ... )) # this is the tricky part.\n","            # Resolve nested tuples in the lambda to get (filename, words / vocab) tuples\n","print('Average word occurrences: ',f_awo_RDD.take(3))\n","# should look like this [('henry_V', 6.212341574901309), ('macbeth', 5.699808271706382), ('lady_susan', 8.430416532127866)]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aMl4P-NRfXhf"},"source":["\n","## 2) Fixed vectors: Reduced vocabulary approach\n","\n","The first task in this lab is to use a **reduced vocabulary** - **only stopwords** from a list This is a common approach in **stylometry**.\n","It also make sure that we have a **fixed size vector**, which is needed for most machine learning task.\n","\n","A problem is that some **stopwords** might **not appear in some documents**. We will deal with that by creating an RDD with `((f, w), 0)` tuples that we then merge with the `((f, w), count)` RDD.\n","\n","Start by running the code above, then you can add 1s and use `reduceByKey(add)` like last week to get the counts of the words per filename.\n","\n","Then, please make sure that all stopwords are present by creating a new RDD that contains the keys of the fw_RDD, i.e. the filenames, using the `keys()` method of the RDD. Then you can use `flatMap` to create a `[((filename, stopword), 0), ...]` list, using a list comprehension. The 0s should not be 1s, as we don't want add to add extra counts.\n","The RDD with `((filename, stopword), 0)` tuples can then be merged with `fw_RDD2` using `union()`. Then you can count as normal."]},{"cell_type":"code","metadata":{"id":"CTp6WYxdfXhg"},"source":["from operator import add\n","\n","stopwlst = ['the','a','in','of','on','at','for','by','i','you','me'] # stopword list\n",">>>fw_RDD2 = fw_RDD.filter(lambda fw: ... ) # filter, keeping only stopwords\n","\n","# create a list of stopwords with count 0 to avoid missing elements in our vectors\n","fw_0_RDD = fw_RDD.keys().flatMap(lambda f: [((f, sw), 0) for sw in stopwlst])\n","print(fw_0_RDD.take(3))\n","# output should look like this:\n","#[(('emma', 'the'), 0), (('emma', 'a'), 0), (('emma', 'in'), 0)]\n","\n",">>>fw_1_RDD = fw_RDD2.map(lambda fw: ...)  # <<< change (f,w) to ((f,w),1)\n","print(fw_1_RDD.take(3))\n","# output should look like this:\n","#[(('emma', 'the'), 1), (('emma', 'of'), 1), (('emma', 'by'), 1)]\n","\n",">>>fw_10_RDD = fw_1_RDD ... fw_0_RDD # <<< create the union on the two RDDs\n","print(fw_10_RDD.take(3))\n","# output should look like this:\n","#[(('emma', 'the'), 1), (('emma', 'of'), 1), (('emma', 'by'), 1)]\n","\n","fw_c_RDD = fw_10_RDD.reduceByKey(add) # count the words\n","print(fw_c_RDD.take(3))\n","# output should look like this:\n","#[(('emma', 'the'), 5380), (('emma', 'by'), 591), (('emma', 'you'), 2068)]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cbADQcyLfXhk"},"source":["## 3) Creating sorted lists\n","\n","As a next step, map the `((filename, word), count)` to `( filename, [(word, count)])` using the function `reGrpLst` to regroup and create a list.\n","\n","Then sort the `[(word, count), ...]` lists in the values (i.e. 2nd part of the tuple) with the the words as keys. Have a [look at the Python docs](https://docs.python.org/3.6/library/functions.html?#sorted) for how to do this. Hint: use a lambda that extracts the words as the key, e.g. `sorted(f_wcL[1], key = lambda wc: ... )`.   "]},{"cell_type":"code","metadata":{"id":"NZPYL4gcfXhl"},"source":["def reGrpLst(fw_c): # we get a nested tuple\n","    >>>     # split the outer tuple\n","    >>>     # split the inner tuple\n","    return (f,[(w,c)]) # return (f,[(w,c)]) structure. Can be used verbatim, if your variable names match.\n","\n","\n",">>>f_wcL_RDD = fw_c_RDD.map(...) # apply reGrpLst\n","f_wcL2_RDD = f_wcL_RDD.reduceByKey(add) # create [(w,c), ... ,(w,c)] lists per file\n",">>>f_wcLsort_RDD = f_wcL2_RDD.map(lambda f_wcL: (f_wcL[0], sorted(...))) #<<< sort the word count lists by word\n","print(f_wcLsort_RDD.take(3))\n","# output:\n","# [('macbeth', [('a', 395), ('at', 64), ('by', 74), ('for', 142), ('i', 581), ('in', 227), ('me', 119), ('of', 427), ...\n",">>>f_wVec_RDD = f_wcLsort_RDD.map(lambda f_wc: (f_wc[0], [c for ...])) # remove the words from the wc pairs\n","f_wVec_RDD.take(3)\n","# output:\n","# [('macbeth', [395, 64, 74, 142, 581, 227, 119, 427, 76, 765, 272]),\n","#  ('lady_susan', [611, 161, 152, 262, 1106, 402, 200, 787, 140, 784, 353]),\n","#  ('merchant_of_venice', [646, 75, 131, 254, 976, 319, 260, 535, 81, 938, 507])]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4pzz4OdSfXho"},"source":["## 4) Clustering\n","\n","Now we have feature vectors of fixed size and fixed sequential order, we can use KMeans as provided by Spark.\n","\n","The files in our library are by two authors. After clustering, check if the clusters reflect authorship:\n","\n","WILLIAM SHAKESPEARE:\n","merchant_of_venice,\n","richard_III,\n","midsummer,\n","tempest,\n","romeoandjuliet,\n","othello,\n","henry_V,\n","macbeth,\n","king_lear,\n","julius_cesar,\n","hamlet\n","\n","JANE AUSTEN:\n","mansfield_park,\n","emma,\n","northanger_abbey,\n","lady_susan,\n","persuasion,\n","prideandprejudice,\n","senseandsensibility"]},{"cell_type":"code","metadata":{"id":"dT3vsHxDfXhp"},"source":["from math import sqrt\n","from pyspark.mllib.clustering import KMeans\n","\n",">>>wVec_RDD = f_wVec_RDD.map(lambda f_wcl: ...) # strip the filenames, keep only the vectors\n","\n","# Build the model (cluster the data)\n","clusterModel = KMeans.train(wVec_RDD, 2, maxIterations=10, initializationMode=\"random\")\n","\n","# Assign the filenames to the clusters\n","fc_RDD = f_wVec_RDD.map(lambda fv: (fv[0],clusterModel.predict(fv[1])))\n","for s in fc_RDD.collect():\n","    print(s)\n","\n","# Evaluate clustering by computing Within Set Sum of Squared Errors\n","def error(point):\n","    center = clusterModel.centers[clusterModel.predict(point)]\n","    return sqrt(sum([x**2 for x in (point - center)]))\n","\n","WSSSE = wVec_RDD.map(lambda point: error(point)).reduce(lambda x, y: x + y)\n","print(\"Within Set Sum of Squared Error = \" + str(WSSSE))\n","# now check if the clusters match the authors\n","# output:\n","# ('macbeth', 0)\n","# ('lady_susan', 0)\n","# ('merchant_of_venice', 0)\n","# ('othello', 0)\n","# ('persuasion', 1)\n","# ('emma', 1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iK7V6FuzfXhs"},"source":["## 5) Alternative approach: feature hashing\n","\n","Instead of the previous approach, we now use feature hashing, as done last week."]},{"cell_type":"code","metadata":{"id":"ENw0xiFRfXht"},"source":["def hashing_vectorizer(word_count_list, N):\n","    v = [0] * N  # create fixed size vector of 0s\n","    for word_count in word_count_list:\n",">>>        ... # unpack tuple\n","        h = hash(word) # get hash value\n","        v[h % N] = v[h % N] + count # add count\n","    return v # return hashed word vector\n","\n","from operator import add\n","\n","N = 10\n","\n","# we use fw_RDD from the beginning with all the words, not just stopwords\n",">>>fw_1_RDD = fw_RDD.map(lambda fw: ...)  # <<< change (f,w) to ((f,w),1)\n","fw_c_RDD = fw_1_RDD.reduceByKey(add) # as above\n","f_wcL_RDD = fw_c_RDD.map(reGrpLst) # as above\n","f_wcL2_RDD = f_wcL_RDD.reduceByKey(add) # create [(w,c), ... ,(w,c)] lists per file\n",">>>f_hwVec_RDD = f_wcL2_RDD.map(lambda f_wcL: (f_wcL[0], ...)) # apply the hashing_vectorizer to the word-count list\n","print(f_hwVec_RDD.take(3))\n","# output:\n","# [('henry_V', [3176, 3044, 2429, 2813, 2947, 3257, 2144, 4706, 1823, 3561]), ('macbeth', [1962, 2023, 1875, 2145, 2011, 2238, 1610, 3166, 1343, 2437]), ('lady_susan', [3491, 2624, 1871, 2726, 2847, 2896, 1967, 3847, 1179, 2661])]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QNdwVx8tfXhw"},"source":["from math import sqrt\n","from pyspark.mllib.clustering import KMeans\n","\n","hwVec_RDD = f_hwVec_RDD.map(lambda f_wcl: f_wcl[1]) # strip the filenames\n","\n","# Build the model (cluster the data)\n","clusterModel = KMeans.train(hwVec_RDD, 2, maxIterations=10, initializationMode=\"random\")\n","\n","# Assign the files to the clusters\n","fhc_RDD = f_hwVec_RDD.map(lambda fv: (fv[0],clusterModel.predict(fv[1])))\n","for s in fhc_RDD.collect():\n","    print(s)\n","\n","# resusing 'error' function from above\n","WSSSE = hwVec_RDD.map(lambda point: error(point)).reduce(lambda x, y: x + y)\n","print(\"Within-Set Sum of Squared Error = \" + str(WSSSE))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"fJWcRwgvfXhz"},"source":["## 6) Compensating document length: normalised vectors\n","\n","**'Lady Susan'** ends up reliably in the **wrong cluster**. A possible explanation could be that it is **shorter** than the other Austen works. Try **normalising** the word counts, i.e. by dividing by their sum. That takes away the effect of length. Is there and effect on the clustering?\n","    \n","You can use a list comprehension for the normalisation."]},{"cell_type":"code","metadata":{"id":"UsXFZvl0fXhz"},"source":[">>>nwVec_RDD = wVec_RDD.map(lambda v: ...) # provide a list comprehension that\n","                            # normalises the values by dividing by the sum over the list\n","print(\"Normalised vectors: \", nwVec_RDD.take(3))\n","# output:\n","# Normalised vectors:  [[0.12563613231552162, 0.020356234096692113, 0.023536895674300253, 0.045165394402035625, 0.18479643765903309, ...\n","\n","# Build the model (cluster the data)\n","clusterModel = KMeans.train(nwVec_RDD, 2, maxIterations=10, initializationMode=\"random\")\n","\n","# Assign the files to the clusters\n","fnc_RDD = f_wVec_RDD.map(lambda fv: (fv[0], clusterModel.predict(fv[1])))\n","for s in fnc_RDD.collect():\n","    print(s)\n","# output\n","# ('macbeth', 0)\n","# ('lady_susan', 0)\n","# ('merchant_of_venice', 0)\n","# .."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mC93IHTO7XCP"},"source":["**Comment:** Unfortunately, there is no positive effect on the classification of lady_susan. We'll need to use another method, e.g. supervised learning, to determine authorship."]},{"cell_type":"markdown","metadata":{"id":"ZybF9l6PfXh2"},"source":["## 7) Building an index\n","\n","Starting from the fw_RDD we now start **building the index** and calculating the **IDF values**. Since we have the TF values already, we only need to keep the unique filenames per word using [RDD.distinct()](https://spark.apache.org/docs/2.4.4/api/python/pyspark.html#pyspark.RDD.distinct).  \n","Then we create a list of filenames. The length of the list is the **document frequency DF** per word.\n","From the DF value we can calculate the **IDF** value as **`log(num_documents/DF)`**."]},{"cell_type":"code","metadata":{"id":"-6ZOrORDfXh3"},"source":["from math import log\n","\n","num_documents = ft_RDD.count()\n","\n","fwu_RDD = fw_RDD.distinct() # get unique file/word pairs\n",">>>wfl_RDD = fwu_RDD.map(lambda fw: (fw[1], ...)) # create (w, [f]) tuples\n","wfL_RDD = wfl_RDD.reduceByKey(add) # concatenate the lists with 'add'\n","print(wfL_RDD.take(3))\n","# output:\n","# [('of', ['henry_V', 'macbeth', 'lady_susan', 'midsummer', 'merchant_of_venice', 'king_lear', 'northanger_abbey', 'othello', ...\n","\n",">>>wdf_RDD = wfL_RDD.map(lambda wfl: (wfl[0], ...)) # get the DF replacing the file list with its length\n","print(\"DF: \",wdf_RDD.take(3))\n","# output:\n","# DF:  [('of', 18), ('shakespeare', 15), ('henry', 9)]\n","\n",">>>widf_RDD = wdf_RDD.map(lambda wdf: (wdf[0], ...)) # get the IDF by replacing DF with log(num_documents/DF)\n","print(\"IDF: \",widf_RDD.take(3))\n","# output:\n","# IDF:  [('of', 0.0), ('shakespeare', 0.1823215567939546), ('henry', 0.6931471805599453)]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aX6EvpiZbqih"},"source":["We will work more with IDF values next week."]}]}