{"cells":[{"cell_type":"markdown","metadata":{"id":"NjRMM_hV3qpj"},"source":["# Lab Sheet 5a: N-Gram Classification and more Google Cloud\n","\n","These tasks are for working in the lab session and during the week.\n","\n","We'll revisit some of the tasks of the previous weeks, and add some classification with **n-grams**. Specifically, we'll have a look at using n-grams to represent the novels of lab 3 and the newsgroups of lab 4. In both cases, we then calculate hashed feature vectors from the n-grams, similar to what we've been doing before. These vectors are then stored in a DataFrame, which we use to train a classifier with like in lab 4. In essence, only the preprocessing changes, but the resulting document representations will take the word context as well.\n","\n","We'll also continue to use Google **Cloud**.\n","\n","First we mount drive and install local Spark as usual."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a8J4OnrM4oHc"},"outputs":[],"source":["# Load the Drive helper and mount\n","from google.colab import drive\n","\n","# This will prompt for authorization.\n","drive.mount('/content/drive')\n","\n","# install spark\n","%cd\n","!apt-get update -qq\n","!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n","!tar -xzf \"/content/drive/My Drive/Big_Data/data/spark/spark-3.5.0-bin-hadoop3.tgz\"\n","!pip install -q findspark\n","import os\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n","os.environ[\"SPARK_HOME\"] = \"/root/spark-3.5.0-bin-hadoop3\"\n","import findspark\n","findspark.init()\n","%cd /content\n"]},{"cell_type":"markdown","metadata":{"id":"wsyiHt2WHuJf"},"source":["Now lets run spark and get a context."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WXpA-FuQHy7a"},"outputs":[],"source":["import pyspark\n","# get a spark context\n","sc = pyspark.SparkContext.getOrCreate()\n","print(sc)\n","# and a spark session\n","spark = pyspark.sql.SparkSession.builder.getOrCreate()\n","print(spark)"]},{"cell_type":"markdown","metadata":{"id":"QRTgVdB9FE6t"},"source":["## 1) Creating n-grams from novels"]},{"cell_type":"markdown","metadata":{"id":"3NVve56E3qpm"},"source":["Remember that we tried to cluster novels by author in lab 3, based on their usage of stopwords. Now we will read the entire content of the novels and look at the words in the context of their preceding words. After that we again convert the n-grams to hashed feature vectors, now using the authors' word n-gram frequency instead of stopword frequency.\n","\n","Here is **code from lab 3** that we run first, and then extend."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CVbLZ-PM3qpo"},"outputs":[],"source":["import re\n","from operator import add\n","\n","def stripFinalS( word ):\n","    word = word.lower() # lower case\n","    if len(word) >0 and word[-1] == 's': # check for final letter\n","        return word[:-1]\n","    else:\n","        return word\n","\n","def splitFileWords(filenameContent): # your splitting function\n","    f,c = filenameContent # split the input tuple\n","    fwLst = [] # the new list for (filename,word) tuples\n","    wLst = re.split('\\W+',c) # <<< now create a word list wLst\n","    for w in wLst: # iterate through the list\n","        fwLst.append((f,stripFinalS(w))) # <<< and append (f,w) to the\n","    return fwLst # return a list of (f,w) tuples\n","\n","def hashing_vectorizer(word_count_list, N):\n","     v = [0] * N  # create fixed size vector of 0s\n","     for word_count in word_count_list:\n","        word, count = word_count \t# unpack tuple\n","        h = hash(word) # get hash value\n","        v[h % N] = v[h % N] + count # add count\n","     return v # return hashed word vector\n","\n","def reGrpLst(fw_c): # we get a nested tuple\n","    fw,c = fw_c\n","    f,w = fw\n","    return (f,[(w,c)]) # return (f,[(w,c)]) structure."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Bxc2x6FLWCzp"},"outputs":[],"source":["dirPath = '/content/drive/My Drive/Big_Data/data/library/'\n",">>>ft_RDD = ... #<<< add code to create an RDD with wholeTextFiles"]},{"cell_type":"markdown","metadata":{"id":"F8eYFrx-3qps"},"source":["### a) Create n-grams and n-gram frequency vectors\n","\n","Now modify the `splitFileWords` function to **extract n-grams** instead of words. More specifically, it extracts *unigrams, bigrams, ... , n-grams*, i.e. *k*-grams for each *k* ranging from 1 to the provided *n*. For this you need to have a variable for the n-gram's start and one for its end point. **The logic is given** in the first version with nested list comprehensions. Please **translate it into normal for-loops** for the second version, as an exercise in understanding Pythonic data processing.\n","\n","Then we need a **function to manage the file names** to go with the n-grams.\n","\n","Finally we can **use this in RDD transformations**. A nice trick is the so-called **currying** or **partial parametrisation** of the function with the Python functools. The code is provided, the documentation to understand what is going on can be found at [https://docs.python.org/3/library/functools.html#functools.partial](https://docs.python.org/3/library/functools.html#functools.partial)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CeY39jDl3qps"},"outputs":[],"source":["# Using a nested list comprehension to create 1..n-grams given a string.\n","def split1NGrams(text, n): # function for splitting a word list and creating n-grams\n","    nGramLst = [] # the new list for (filename, word) tuples\n","    wLst = re.split('\\W+', text) # now create a word list wLst\n","    wLst = list(map(stripFinalS, wLst)) # remove final s from the word list (this is a local map, don't confuse with RDD or DF map)\n","    wNum = len(wLst) # get total length to avoid overrunning at the end.\n","    nGramLst = [' '.join(wLst[i:j]) for i in range(wNum) for j in range(i+1, min(wNum, i+n+1))]\n","    return nGramLst # return a list of (f, w) tuples\n","\n","# Alternative version with separate function for converting a word-list to n-grams\n","def lst21ngram(wLst, n):\n","    wNum = len(wLst) # get total length to avoid overrunning at the end.\n","    nGramLst = [] # output list\n","    # <<< reprogram the nested list comprehension above with regular for loops.\n","    ...\n","    return nGramLst # done\n","\n","# a wrapper around the separate function\n","def split1NGrams2(text, n): # your splitting function\n","    wLst = re.split('\\W+', text) #  split into words\n","    nGramLst = lst21ngram(wLst,n) # create the n-grams\n","    return nGramLst # done\n","\n","# This function manages the filenames around the 1..n-gram extraction\n","def splitFile1NGrams(filenameContent, n=2): # your splitting function\n","    f, c = filenameContent # split the input tuple\n","    ngLst = split1NGrams2(c,n) # split the file content into n-grams\n","    fngLst = [] # the new list for (filename, n-gram) tuples\n","    for ng in ngLst: # iterate through the list\n","        fngLst.append((f,ng)) # and append (f, ng) to the new list\n","    return fngLst # return a list of (f,ng) tuples\n","\n","# just for testing\n","print(split1NGrams('a b c d e f g', 3)) # test the splitting function with a string (easier than with an RDD or DF)\n","print(split1NGrams2('a b c d e f g', 3))# test the 2nd version, should look like the first\n","print(splitFile1NGrams(('file','a b c d e f g'), 3)) # should add the file tag before the n-grams"]},{"cell_type":"markdown","metadata":{"id":"3aYT3GulVZKE"},"source":["Now let's **use the new functions** to create RDDs with n-gram vectors:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OmXm_V2lVWZR"},"outputs":[],"source":["from functools import partial\n","# use a partial to define the max len of the n-grams\n","fng_RDD = ft_RDD.flatMap(partial(splitFile1NGrams, n=2)) # <<< read the documentation (link above) to figure out what happens here\n","fng_RDD.take(5)\n",">>>fng_1_RDD = fng_RDD.map(...)  # <<< like in lab 3, as an exercise for the reader ;-) change (f, ng) to ((f, ng), 1)\n",">>>fng_c_RDD = fng_1_RDD.reduceByKey(...) # <<< like in lab 3, as an exercise for the reader ;-) add the ones\n","f_ngcL_RDD = fng_c_RDD.map(reGrpLst) # regroup to (f, [(ng, c)])\n","f_ngcL2_RDD = f_ngcL_RDD.reduceByKey(add) # concatenate ngram counts into one list per file\n","f_ngVec_RDD = f_ngcL2_RDD.map(lambda f_wc: (f_wc[0], hashing_vectorizer(f_wc[1], 10))) # we can apply the vectorizer as normal\n","print(f_ngVec_RDD.take(3))"]},{"cell_type":"markdown","metadata":{"id":"1le7gtZp3qpz"},"source":["### b) Convert n-gram RDD into DataFrame\n","\n","The next task is to **create a DataFrame from the RDD**. This is similar to what was shown in lab 4 and also to the documentation: [http://spark.apache.org/docs/latest/sql-programming-guide.html#interoperating-with-rdds](http://spark.apache.org/docs/latest/sql-programming-guide.html#interoperating-with-rdds)  \n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2TJ2m9LJ3qpz"},"outputs":[],"source":["austen_novels = ['senseandsensibility.txt', 'mansfield_park.txt', 'emma.txt', 'persuasion.txt', 'northanger_abbey.txt', 'lady_susan.txt', 'prideandpredjudice.txt']\n","austen = ['file:/content/drive/MyDrive/Big_Data/data/library/'+s for s in austen_novels]\n","print(austen)\n","\n","av_RDD = f_ngVec_RDD.map(lambda f_wVec: ('Austen' if (f_wVec[0] in austen) else 'Shakespeare', f_wVec[1]))\n","\n","from pyspark.sql import Row\n","\n",">>>row_RDD = av_RDD.map(...Row(author ..., vector ...)) # <<< create a Row objects (similar to LabelledPoints)\n","# Create a dataFrame from the RDD\n",">>>library_DF = spark. .... # create the data frame, like in lab 4, but without giving an explicit schema\n","library_DF.createOrReplaceTempView(\"library\")\n",">>> ... # print the schema\n",">>> ... # show the first 5 elements"]},{"cell_type":"markdown","metadata":{"id":"RNRIaRkbIbBS"},"source":["Remember from lab 4 that SQL can be used over DataFrames that have been registered as a table."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YScvxpLJbrAU"},"outputs":[],"source":["SQL1 = \"SELECT author,vector FROM library WHERE author=='Austen'\"\n","austen_vectors = spark.sql(SQL1)\n","print(SQL1)\n","austen_vectors.show()\n","\n","# create an SQL query that gives you only the authors that are not called Austen\n",">>>SQL2 = ...\n","other_vectors = spark.sql(SQL2)\n","print(SQL2)\n","other_vectors.show()"]},{"cell_type":"markdown","metadata":{"id":"9Rt9bri8X2Uh"},"source":["## 2) Running PySpark on Google's Cloud Platform\n","\n","We have started using Google Cloud in the last lab. Now let's port the solution for task 1a) above to the cloud.\n","\n","Open the notebook 'Running Spark in the Google' cloud from week 4 as a reference. You can copy and past code from there."]},{"cell_type":"markdown","metadata":{"id":"O4T9I7fXH9i5"},"source":["### a) cloud setup\n","The first step is to authenticate, the exact method varies per platform. For Colab, we use the 'google.colab.auth' package, as in the last lab.  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ekJTPS5NIRln"},"outputs":[],"source":["import sys\n","if 'google.colab' in sys.modules:\n","    from google.colab import auth\n","    auth.authenticate_user()"]},{"cell_type":"markdown","metadata":{"id":"lMxxavWdIckY"},"source":["Then we create the project and region variables and set the values for this cloud session."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tFZQ1GXkIcMW"},"outputs":[],"source":["### this project NEEDS TO BE SET UP IN GOOGLE CLOUD FIRST\n","PROJECT = 'big-data-cw22' ### Append -xxxx, where xxxx is your City login to make project names unique ###\n","### it seems that the project name here has the be in lower case.\n","!gcloud config set project $PROJECT\n","REGION = 'us-central1' # this has worked most reliably with the free tier\n","!gcloud config set compute/region $REGION\n","!gcloud config set dataproc/region $REGION"]},{"cell_type":"markdown","metadata":{"id":"ldI8BW9xIxSJ"},"source":["Then we create a bucket, using the code from last time."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xv189Gj3I1Bc"},"outputs":[],"source":["BUCKET = 'gs://{}-storage'.format(PROJECT)\n","!gsutil mb $BUCKET"]},{"cell_type":"markdown","metadata":{"id":"UZ2l5AAII5kq"},"source":["### b) copy the data\n","With gsutil, we can use commands similar to the unix shell to copy data to the bucket. The 'cp' command copies data and we can use the glob pattern '*' to match all files in a directory.\n","\n","One difference between cloud buckets and local file systems is that directories (like 'library') are not objects in a bucket but instead are treated like parts of the filename. Therefore we cannot create the target directory in the bucket, but specify it as part of the target path, even when it doesn't exist, yet.    "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hZlxfmm4I97_"},"outputs":[],"source":["!ls '/content/drive/My Drive/Big_Data/data/library/'\n","\n","!gsutil -m cp '/content/drive/My Drive/Big_Data/data/library/*' $BUCKET/library"]},{"cell_type":"markdown","metadata":{"id":"2gqknXysJCFa"},"source":["### c) create the cluster\n","\n","Like in the last lab, we create the cluster with a unique name using the `gcloud cluster create` command. See [here](https://cloud.google.com/sdk/gcloud/reference/dataproc/clusters/create) for documentation\n","\n","This time, let's create a cluster with a master and 3 worker machines. For that, copy the `gcloud cluster create  ...` code from the last lab.\n","\n","Remove the `--single-node` flag and use `--num-workers` instead to request 3 worker machines. Specifying the boot disk type and size fort the is analogous to the master, i.e. just copy the flags and replace 'master' with 'worker'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XGyYj1DzJKHV"},"outputs":[],"source":["CLUSTER = '{}-cluster'.format(PROJECT)\n","\n","#!gcloud dataproc clusters create $CLUSTER \\\n","#    --image-version 1.5-ubuntu18 --single-node \\\n","#    --master-machine-type n1-standard-2 \\\n","#    --master-boot-disk-type pd-ssd --master-boot-disk-size 100 \\\n","#    --max-idle 3600s\n","#>>> adapt the code above to create a cluster with 3 workers"]},{"cell_type":"markdown","metadata":{"id":"_aG1LlLQJX6u"},"source":["Check the [console Dataproc page](https://console.cloud.google.com/dataproc/clusters/) to see that the cluster is running.\n"]},{"cell_type":"markdown","metadata":{"id":"qCNinQqaJbuV"},"source":["### d) create the script\n","\n","For this, you need to combine the code cells under 1a) and the ones above (except for the code mounting drive and installing spark). All code needs to go into one code cell and you need to write the content into a file using the `%%writefile <filename>` magic at the beginning of the code cell.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W-HP8uFtJ8eR"},"outputs":[],"source":["%%writefile script.py\n","#>>> copy all the from 1a) and before here (except drive mounting and spark installation)."]},{"cell_type":"markdown","metadata":{"id":"5SQcGJ50KTso"},"source":["Once we have the script file, we can submit it to our Spark cluster using `gcloud submit`.\n","\n","You will get a lot of output, but among all the log messages, you should find the same output as before."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zduE9AmhKX1A"},"outputs":[],"source":["#>>> use gcloud submit to run your script in the cloud"]},{"cell_type":"markdown","metadata":{"id":"uhEGD-KtMO7y"},"source":["Check the output and compare with 1a).\n","\n","Have a look at the [dataproc page on the cloud console](https://console.cloud.google.com/dataproc/clusters) to see how the machines are used."]},{"cell_type":"markdown","metadata":{"id":"fQ7inlTNI3dR"},"source":["# Extra tasks (optional)\n"]},{"cell_type":"markdown","metadata":{"id":"Wo7NB9k4Nuib"},"source":["## 3) Run more tasks in the cloud\n","\n","Try running other tasks from this and previous labs in the cloud."]},{"cell_type":"markdown","metadata":{"id":"tSoB4jG13qp3"},"source":["## 4) Creating n-grams from the newsgroups dataset\n","\n","To use the newsgroups dataset from lab 4, we need to parse the messages like we did before."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9xAmkJBk3qp4"},"outputs":[],"source":["import re\n","import os.path\n","\n","p = '/content/drive/MyDrive/Big_Data/data/20_newsgroups/'\n","\n","#here we are setting the path to select 2 topics\n","dirPath1 = os.path.join(p, 'alt.atheism')\n","dirPath2 = os.path.join(p, 'comp.graphics')\n","\n","# remove the headers, get the sender and the main text\n","def parseMessage(ft):\n","    fn, text = ft # unpack the filename and text content\n","    # now use a regular expression to match the text\n","    # When you check the data, you can see that the first line that starts with 'Lines:' normally ends the header.\n","    # Only the very first file is different, but we can tolerate one wrong sample for now.\n","    # (How could we be more thorough?)\n","    matchObj = re.search(r'.+^(Lines:|NNTP-Posting-Host:) (.*)', text, re.DOTALL|re.MULTILINE|re.IGNORECASE)\n","    if(matchObj): # only if the pattern has matched\n","        text = matchObj.group(2) # can we replace the text\n","    else:\n","        text = \"\" # otherwise we return an empty string, in order to avoid giving header information to the model, which would give away the class.\n","    return (fn, text)\n","\n","# for testing the parseMessages function\n","#ft_RDD = sc.wholeTextFiles(dirPath1) # create an RDD with wholeTextFiles\n","#txts = ft_RDD.take(3) # take into a local list\n","#txts2 = list(map(parseMessage, txts))# and apply removeHeader (NOTE: this is different from an RDD map!)\n","#print(txts2)"]},{"cell_type":"markdown","metadata":{"id":"2kvBo7F-N2MJ"},"source":["We then add the function `splitFile1NGrams` created above to the preprocessing pipeline."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q1pNLW9EfwoM"},"outputs":[],"source":["# we need to create our feature vectors using the pyspark.ml.linalg.DenseVector class,\n","# in order to use the CrossValidation later\n","from pyspark.ml.linalg import DenseVector\n","\n","# Make a DataFrame with labels and N-gram vectors\n","def make_dataFrame(dirPath, argLabel, N, NG):\n","    print(\"make_dataFrame started\")\n","    ft_RDD = sc.wholeTextFiles(dirPath) # create an RDD with wholeTextFiles\n","    ft2_RDD = ft_RDD.map(parseMessage) # parse the messages\n","    # print(\"ft2_RDD.take(2)\", ft2_RDD.take(2))\n","    >>>fng_RDD = ft2_RDD.flatMap(...) # split the file with a 'partial' like in the task 1, fixing the n-gram parameter to \"NG\".\n","    # print(\"fng_RDD.take(2)\", fng_RDD.take(2))\n","    print(\"fng_RDD.count()\", fng_RDD.count())\n","    fng2_RDD = fng_RDD.filter(lambda x: x is not None) # filter files we couldn't parse\n","    print(\"fng2_RDD.count()\", fng2_RDD.count())\n","    fng_1_RDD = fng2_RDD.map(lambda x: (x, 1))  # change (fs, ng) to ((fs, ng), 1) - we can ignore that (fs, ng) actually is a tuple here\n","    fng_c_RDD = fng_1_RDD.reduceByKey(add) # as above\n","    f_ngcL_RDD = fng_c_RDD.map(reGrpLst) # as above\n","    f_ngcL2_RDD = f_ngcL_RDD.reduceByKey(add) # create [(ng, c), ..., (ng, c)] lists per file\n","    f_ngVec_RDD = f_ngcL2_RDD.map(lambda f_wc: (f_wc[0], hashing_vectorizer(f_wc[1], N)))\n","    # <<< below create a Row with dense vectors and the argLabel called 'features' and 'label'\n","    # <<< convert your list of hashed features to a DenseVector in order to make them compatible with the pyspark.ml library\n","    # <<< you can just call DenseVector(list) to achieve this\n","    >>>rows_RDD = f_ngVec_RDD.map(... Row(label= ..., features= ...))\n","    rows_DF = spark.createDataFrame(rows_RDD)\n","    return rows_DF\n","\n","N  = 10 # vector size\n","NG =  3 # max n-gram size\n","\n","rows1_DF = make_dataFrame(dirPath1, 0, N, NG)\n","rows2_DF = make_dataFrame(dirPath2, 1, N, NG)\n","rows_DF = rows1_DF.union(rows2_DF)\n","rows_DF.createOrReplaceTempView(\"newsgroups\")\n","rows_DF.cache()\n","print(rows_DF.count())\n","rows_DF.printSchema()\n","rows_DF.show(5)"]},{"cell_type":"markdown","metadata":{"id":"iAlenysl3qp7"},"source":["## 5) Use the spark.ml cross-validator\n","\n","Now we can use the **CrossValidator**, which comes with the `pyspark.ml` module on the newsgroup DataFrames. We only need to set up the parameters. Have a look at the extra task of lab 4 for hints."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lph6sFUv3qp8"},"outputs":[],"source":["# We need to import the classifiers from the ML package now.\n","from pyspark.ml.classification import NaiveBayes\n","\n","# The CrossValidator and ParamGridBuilder enable the automatic tuning\n","from pyspark.ml.tuning import CrossValidator\n","from pyspark.ml.tuning import ParamGridBuilder\n","\n","# The evaluator to test the model\n","from pyspark.ml.evaluation import BinaryClassificationEvaluator\n","\n","nb = NaiveBayes()\n","# <<< build a parameter grid for the nb.smoothing value\n","evaluator = BinaryClassificationEvaluator()\n","print(\"starting cross-validation\")\n",">>>cv = CrossValidator(estimator= ... , estimatorParamMaps= ... , evaluator= ... ) # <<< fill in the correct values\n","cvModel = cv.fit(rows_DF)\n","print(\"finished cross-validation\")\n","# <<< add evaluation and parameter values"]}],"metadata":{"colab":{"private_outputs":true,"provenance":[{"file_id":"1D2uWYz-gEM87Q-vuWGYUSCxEWPcg-_FU","timestamp":1583453612378},{"file_id":"1qn6gxkC0p_YTQhf5QlBgAjVPfXhXOluy","timestamp":1582915559333},{"file_id":"1FKkzCAYXRVzrIKsFl4HiKcCVbdCgsBOp","timestamp":1552258234691},{"file_id":"1-V0o0KH07oyNi3r80s5RFLlIXC2lSmhG","timestamp":1552042722818}]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":0}