{"cells":[{"cell_type":"markdown","metadata":{"id":"41I6gcRAE3UB"},"source":["# Lab Sheet 4a:  Text Classification with DataFrames and getting started with Google Cloud\n","\n","These tasks are for working in the lab session and during the week.\n","\n","We'll build on last week's code and add  \n","* Classification\n","* Execution Timing\n","* DataFrames\n","* Spark ML\n","* ML Pipeline\n","* Evaluation and HP tutning\n","\n","We are going to use a **simple text classification problem** to address all these points.\n","\n","In addition, we'll start using Google Cloud."]},{"cell_type":"markdown","metadata":{"id":"jM7CA9LhedX-"},"source":["## 1) Preparation\n","As usual, we start by mounting Google Drive, then installing and starting Spark."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_-Wya1GfGT12"},"outputs":[],"source":["# Load the Drive helper and mount\n","from google.colab import drive\n","\n","# This will prompt for authorization.\n","drive.mount('/content/drive')\n","\n","%cd\n","!tar -xzf \"/content/drive/My Drive/Big_Data/data/spark/spark-3.5.0-bin-hadoop3.tgz\" # unpacking\n","!apt-get install openjdk-8-jdk-headless -qq > /dev/null # installing java\n","import os # Python package for interaction with the operating system\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\" # tell the system where Java lives\n","os.environ[\"SPARK_HOME\"] = \"/root/spark-3.5.0-bin-hadoop3\" # and where spark lives\n","!pip install -q findspark # install helper package\n","import findspark # use the helper package\n","findspark.init() # to set up spark\n","%cd \"/content/drive/My Drive/Big_Data\"\n","\n","import pyspark\n","# get a spark context for RDDs\n","sc = pyspark.SparkContext.getOrCreate()\n","print(sc)\n","# and a spark session for DataFrames\n","spark = pyspark.sql.SparkSession.builder.getOrCreate()\n","print(spark)"]},{"cell_type":"markdown","metadata":{"id":"FuCii1qhE3UD"},"source":["## 2) Dataset: Newsgroups\n","\n","Today, we use another, **larger dataset**, which consists of \"Usenet\" discussions from the early days of the Internet. This dataset contains messages from 20 different newsgroups on different topics with ~1000 messages each. More information and the data can be found here:\n","[https://archive.ics.uci.edu/dataset/113/twenty+newsgroups](https://archive.ics.uci.edu/dataset/113/twenty+newsgroups)\n","\n","With the larger dataset you should get more meaningful time measurements. However, we also need to wait longer. Try several runs and try executing things in different order (here we are only training a Logistic Regression classifier, however you are encouraged to try other models).\n","\n","To create a meaningful dataset for classification, you need to read in at least 2 topics and then use `RDD.randomSplit()` to create a train and test set. For this lab, we will use alt.atheism and comp.graphics. Try adding more topics to the dataset, there are 20 differet directories (i.e. topics)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l34l95KYE3UQ"},"outputs":[],"source":["# show directory content\n","%ls \"/content/drive/MyDrive/Big_Data/data/20_newsgroups\""]},{"cell_type":"markdown","metadata":{"id":"CSg-J7A0uUaK"},"source":["We can now read the text files into RDDs and create our small trial dataset."]},{"cell_type":"markdown","source":[],"metadata":{"id":"Zi2rzZ9jIDsd"}},{"cell_type":"markdown","metadata":{"id":"be7mIaYWE3Ua"},"source":["## 3) Preprocessing: Extract directory names and remove headers from files with a RegEx\n","\n","We can now read the **text files into RDDs** and create our smaller trial dataset. We use the [`wholeTextFiles()`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.SparkContext.wholeTextFiles.html)  method, which produces tuples in the RDD consisting of the filename and the text contained in the file.  Use the [`use_unicode=False`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.SparkContext.wholeTextFiles.html) option in `wholeTextFiles` to speed up reading.\n","\n","\n","This cell takes a bit of time to execute."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ko0Ht8QJE3UT"},"outputs":[],"source":["%%time\n","import os.path\n","\n","# the dataset directory\n","p = '/content/drive/MyDrive/Big_Data/data/20_newsgroups/'\n","\n","# here we are setting the path to select 2 topics\n","dirPath1 = os.path.join(p, 'alt.atheism')\n","dirPath2 = os.path.join(p, 'comp.graphics')\n","\n","# Use wholeTextFiles to read both the files and make sure you set the\n","# keyword argument 'use_unicode' to False as it speeds up reading dramatically.\n","# Check the link in the cell above for a bit more information.\n",">>> alt_rdd = ...\n","print(alt_rdd.take(1))\n",">>> comp_rdd = ...\n","print(comp_rdd.take(1))\n","\n","# Create a union of the 2 RDD's so we hava a set with both classes\n",">>> newsGroup_RDD_b = alt_rdd ... comp_rdd\n","\n","# The byte strings need to be 'decoded' into standard Python strings.\n","# (Strangely doing it after reading is much faster than reading unicode, not sure why that is ...)\n","# Apply .decode() to *each* item of the tuple containing (filepath, text) separately\n",">>> newsGroup_RDD = newsGroup_RDD_b.map( ... )\n","# print the total number of documents here:\n","print('Number of documents read is:', newsGroup_RDD.count())\n","print(newsGroup_RDD.take(3)) # print out 3 examples.\n","# The 2nd part of the tuple (at address 1) will contain the whole message.\n","# the print command is necessary here, because the %%time magic\n","# prevents the last element from being printed automatically"]},{"cell_type":"markdown","metadata":{"id":"sDGIp78fiq_T"},"source":["You'll see low CPU time (in the millisecond range), but longer wall time (around a minute). What could be the reason?\n","\n","Next, use a Regular Expression with `re.split()` on the file path. We want only the last directory name (e.g. \"sci.space\", these are our class labels), but not the filename and the rest of the path. The approach is to split at the directory separator (`/` on Linux and Mac), and then use the element before the last.\n","See here: https://docs.python.org/3/library/re.html#re.split\n","\n","This can be written in a lambda expression, that keeps the text (the file content), but processes the filepath at position 0 of the tuple."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AWzjm5KRE3UX"},"outputs":[],"source":["import re\n","\n","# Remove the file name and path before the last directory name (i.e. the newsgroup name)\n",">>> fnt_RDD = newsGroup_RDD.map(lambda ft: ...) # extract the newsgroup name from the filepath, keep the text\n","\n","# check the output against the directoy names listed before section 2.\n","fnt_RDD.take(3)"]},{"cell_type":"markdown","metadata":{"id":"rm1W5ef0e9Aq"},"source":["At closer inspection, we can see that the messages have headers, and one of them starts with `Newsgroups:` and actually lists the topic. This is clearly an **unreasonable** shortcut for the classifier, as we are interested in **predicting topics from the text**.\n","\n","Thus, the dataset needs preprocessing to **remove these headers**. We can use a **regular expression** to retain only the actual content of the messages.\n","\n","When you check the data, you can see that the first line that starts with `Lines:` normally ends the headers. Only the very first file is different, but we can tolerate one wrong sample for now. (How could we be more thorough?)\n","\n","It's best to use `re.search()` here. Since we want to match **multiple lines of text**, we need to use `re.DOTALL` and `re.MULTILINE`. We alse need to create **groups** in the expression with brackets `()`, that are then available from the **match object** that gets returned.\n","\n","See here:  \\\n","https://docs.python.org/3/library/re.html#re.search  \\\n","https://docs.python.org/3/library/re.html#match-objects  \\\n","https://docs.python.org/3/library/re.html#re.MULTILINE  \\\n","https://docs.python.org/3/library/re.html#re.DOTALL  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3-4k7ZogE3Uc"},"outputs":[],"source":["import re\n","\n","# new function to remove the headers using regular expressions\n","def removeHeader(ft):\n","    fn, text = ft # unpack the filename and text content\n","    # now use a regular expression to match the text\n",">>>    matchObj = re.search(...)  # fill in the expression here and don't forget to use DOTALL and MULTILINE\n","    if(matchObj): # only if the pattern has matched\n",">>>        text = matchObj.group(...) # can we replace the text. Which element of the matchObj are we looking for?\n","    # otherwise we keep the old for now (what could be a better solution?)\n","    return (fn, text)\n","\n","fnt_RDD2 = fnt_RDD.map(removeHeader)\n","fnt_RDD2.take(3)"]},{"cell_type":"markdown","metadata":{"id":"I53xV_IGE3Uf"},"source":["## 4) DataFrames\n","\n","In this section we will introduce **DataFrames**. To read more about DataFrames look here: \\\n","https://spark.apache.org/docs/latest/sql-programming-guide.html\n","\n","DataFrames can be created by reading directly **from files**, too, such as csv, parquet, or json, but our data had to be preprocessed first.  \n","\n","In Spark we can also create DataFrames **from RDDs** and that is what we will implement in the next section.\n","A DataFrame represents a table structure. We define a schema that contains the names and types of the coumns in the table.\n","\n","From the official documentation:\n","\n","A DataFrame can be created programatically in 3 steps:\n","\n","- Create an RDD of Rows from the original RDD;\n","- Create the schema represented by a StructType matching the structure of Rows in the RDD created in Step 1.\n","- Apply the schema to the RDD of Rows via createDataFrame method provided by SparkSession.\n","\n","More on the **pyspark.sql API** can be found here:\n","\n","https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/index.html\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rs_we7i8E3Ug","pixiedust":{"displayParams":{"handlerId":"barChart","keyFields":"topic"}}},"outputs":[],"source":["from pyspark.sql.types import * # import the type specifications for the schema\n","\n","# The schema is encoded in a string.\n","# Here we are only interested in the topic and text\n","schemaString = \"topic text\"\n","\n","# A StructField object comprises three fields, name (a string), dataType (a DataType)\n","# and a flag indicating whether its value can be 'null' (a bool).\n","# We create 2 fields of strings with names according to our schemaString\n","fields = [StructField(field_name, StringType(), True) for field_name in schemaString.split()]\n","# these together define our schema\n","schema = StructType(fields)\n","\n","# Apply the schema in createDataFrame, to create a DataFrame 'df' from the RDD\n","df = spark.createDataFrame(fnt_RDD2, schema)\n","df.show(3)"]},{"cell_type":"markdown","metadata":{"id":"4YpyrBGvoQDR"},"source":["### Using SparkSQL\n","\n","**SparkSQL** is a limited version of SQL that can be applied to Spark DataFrames. It will be **executed** in a **distributed way**, just like RDD operations.\n","\n","We need to create a (temporary) **`view`** of the DataFrame, so that we can **use SparkSQL**.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_MRIf8YhE3Uk"},"outputs":[],"source":["df.createOrReplaceTempView(\"newsgroups\")\n","\n","# SQL can now be run on the DataFrame.\n","# Let's start by selecting only the topics elements of each row\n","results = spark.sql(\"SELECT DISTINCT topic FROM newsgroups\")\n","results.show()"]},{"cell_type":"markdown","metadata":{"id":"nGoQ1FO7qKH3"},"source":["SparkSQL provides a number of **functions**, a **list** can be found here:\n","http://spark.apache.org/docs/latest/api/sql/index.html \\\n","An **overview** of SparkSQL and DataFrames is provided here:\n","http://spark.apache.org/docs/latest/sql-programming-guide.html"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FaG-mJ1KE3Un"},"outputs":[],"source":["# We can make more sophisticated queries in SQL, e.g. using topic names as a distinct feature and simply count number of files:\n","results_topic = spark.sql(\"SELECT DISTINCT topic, count(*) as count_ FROM newsgroups GROUP BY topic ORDER BY count_ DESC\")\n","results_topic.show()"]},{"cell_type":"markdown","metadata":{"id":"KC5jAWzIE3Ur"},"source":["### Preparing the numeric class labels for classification\n","We need **numeric labels** for the **classifier**. For now, we go for binary labels."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D-ANqr8bE3Ur"},"outputs":[],"source":["# df.withColumn returns a new DataFrame by adding a column with a name and value for each row.\n","# The value is a 'column expression', where we compares with the string 'comp', to find out whether the topic starts with the string \"comp\".\n","# .cast(\"int\") will convert the resulting Boolean value into a number, 1 for 'comp', 0 for other strings.\n","news_Groups = df.withColumn(\"label\", df.topic.like(\"comp%\").cast(\"int\"))\n","\n","# you can use a syntax similar to an array to select some examples of either class\n","alt_topic_df = news_Groups[df.topic.like(\"alt%\")]\n","alt_topic_df.show(3)\n","comp_topic_df = news_Groups[df.topic.like(\"comp%\")]\n","comp_topic_df.show(3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3_HL0jJsE3Uu"},"outputs":[],"source":["# Split the DataFrame into training and test set.\n","# RandomSplit - divides the DataFrame into train/test using the weights given as arguments.\n","# You can try other combinations of weights.\n","train_set, test_set = news_Groups.randomSplit([0.8, 0.2], 123)\n","#>>> now get the sizes of the sets:\n",">>> print(\"Total document count:\", ...)\n",">>> print(\"Training-set count:\", ...)\n",">>> print(\"Test-set count:\", ...)"]},{"cell_type":"markdown","metadata":{"id":"Tq1v9zCAqL7R"},"source":["## 5) Using ML to classify messages"]},{"cell_type":"markdown","metadata":{"id":"gISW3Y55E3Uz"},"source":["### Make an ML Pipeline\n","\n","**ML** is the Spark **machine learning library** for **DataFrames**. We want to build an ML pipeline to predict the Binary label.\n","\n","A Spark ML **Pipeline** is a sequence of stages, and each stage is either a **Transformer** or an **Estimator**. These stages are run in a sequence, and the input DataFrame is transformed as it passes through each stage.\n","\n","Some of the **functions** we implemented ourselves are now used in **ready-made** versions, such as the Hashing Vectorizer or the stopword remover.\n","\n","A practical ML pipeline might consist of many stages like feature extraction, feature transformation, and model fitting. We create a pipeline that consists of the **following stages**:\n","\n","1. **RegexTokenizer** - which tokenizes each article into a sequence of words with a regex pattern,\n","2. **Stop word remover**\n","3. **HashingTF**, maps the word sequences produced by RegexTokenizer to fixed size feature vectors using the hashing trick,\n","4. **IDF**, normalises by the IDF value,\n","5. **LogisticRegression**, which fits the feature vectors and the labels from the training data to a logistic regression model.\n","    \n","To read more on this: https://spark.apache.org/docs/latest/ml-features.html"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tG6DW86EE3U2"},"outputs":[],"source":["from pyspark.ml import Pipeline, PipelineModel\n","from pyspark.ml.classification import LogisticRegression, NaiveBayes\n","from pyspark.ml.feature import HashingTF,StopWordsRemover,IDF,Tokenizer\n","\n","# Constructing a pipeline\n","# We split each sentence into words using Tokenizer.\n","# Tokenizer only splits by white spaces\n","tokenizer = Tokenizer().setInputCol(\"text\").setOutputCol(\"words\") # create a nuem\n","\n","# Remove stopwords\n","remover= StopWordsRemover().setInputCol(\"words\").setOutputCol(\"filtered\").setCaseSensitive(False)\n","\n","# For each sentence (bag of words),use HashingTF to hash the sentence into a feature vector.\n","hashingTF = HashingTF().setNumFeatures(100).setInputCol(\"filtered\").setOutputCol(\"rawFreqs\")\n","\n","# We use IDF to rescale the feature vectors; this generally improves performance when using text as features.\n","idf = IDF().setInputCol(\"rawFreqs\").setOutputCol(\"features\").setMinDocFreq(10)\n","# The Spark ML algorithms expect their input in a column \"featurs\" by default.\n","\n","# Our feature vectors can then be passed to a learning algorithm.\n","lr = LogisticRegression()\n","# nb = NaiveBayes() # you can try this as an alternative to LR (can plot the AUC)\n","\n","# Then we connect all the steps above to create one pipeline\n","pipeline=Pipeline(stages=[tokenizer, remover, hashingTF, idf, lr])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xHW41eqJE3U4"},"outputs":[],"source":["# We can get an information for each parameter  using the .explainParams()\n","print(\"Tokenizer:\", tokenizer.explainParams(),\"\\n\")\n","print(\"Remover:\", remover.explainParams(),\"\\n\")\n","print(\"HashingTF:\", hashingTF.explainParams(),\"\\n\")\n","print(\"IDF:\", idf.explainParams(),\"\\n\")\n","print(\"Pipeline:\", pipeline.explainParams(),\"\\n\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FIM84WR7E3U_"},"outputs":[],"source":["# Use the pipeline option to fit the training set and create a model\n","\n","# After we construct this ML pipeline,we can fit it to the training data\n","# and obtain a trained pipeline model that can be used for prediction.\n","%time model = pipeline.fit(train_set)\n","# %time is a simpler way to take the time than we used in the coursework\n","# Training takes about a minute normally."]},{"cell_type":"markdown","metadata":{"id":"TNmq4TSDE3VC"},"source":["## 6) Evaluate prediction results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N8DaMYJfE3VC"},"outputs":[],"source":["# After we obtain a fitted pipeline model, we want to know how well it performs.\n","# Let us start with some manual checks by displaying the predicted labels.\n","\n","# You can simply use the .transform() on the test set to make predictions on the test set\n","test_predictions = model.transform(test_set)\n","train_predictions = model.transform(train_set)\n","\n","# Show the predicted labels along with true labels and raw texts.\n","test_predictions.select(\"topic\",\"probability\",\"prediction\",\"label\").show(5)\n","# and show some of the other class ...\n","test_predictions.select(\"topic\",\"probability\",\"prediction\",\"label\").filter(test_predictions.topic.like(\"comp%\")).show(5)"]},{"cell_type":"markdown","metadata":{"id":"ffskYVpipsjI"},"source":["### Training set evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gmOejqoME3VF"},"outputs":[],"source":["# The predicted labels look accurate.\n","# Let's evaluate the model quantitatively.\n","from pyspark.ml.evaluation import BinaryClassificationEvaluator\n","\n","evaluatorROC = BinaryClassificationEvaluator().setMetricName(\"areaUnderROC\")\n",">>> evaluatorPR # create an evaluator like above for the 'areaUnderPR', i.e. PrecisionRecall curve\n","train_ROCAUC = evaluatorROC.evaluate(train_predictions)\n",">>> train_PRAUC = # get the PR value\n","print(\"Area under ROC curve - training:\", train_ROCAUC)\n","print(\"Area under PR curve - training:\", train_PRAUC)"]},{"cell_type":"markdown","metadata":{"id":"O9uHlF6-pm26"},"source":["### Test set evaluation\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sPNWrC4sppAi"},"outputs":[],"source":["test_ROCAUC = evaluatorROC.evaluate(test_predictions)\n","test_PRAUC = evaluatorPR.evaluate(test_predictions)\n","print(\"Area under ROC curve - test:\", test_ROCAUC)\n","print(\"Area under PR curve - test:\", test_PRAUC)"]},{"cell_type":"markdown","metadata":{"id":"ZpcALsrrwtMt"},"source":["We can now plot a **ROC curve** with Matplotlib (or another package of your choice). We read the values from the summary that the **evaluator** provides in the **last stage** object."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RNUWCMsLl4mV"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","plt.plot([0, 1], [0, 1], 'k--')\n","plt.plot(model.stages[-1].summary.roc.select('FPR').collect(),\n","         model.stages[-1].summary.roc.select('TPR').collect(),\n","        label='ROC curve (area = {:0.2f})'.format(test_ROCAUC)\n","plt.xlabel('FPR')\n","plt.ylabel('TPR')\n","plt.title('ROC Curve')\n","plt.legend(loc=\"lower right\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OwsQHUmyla_6"},"outputs":[],"source":["plt.plot([0, 1], [1, 0], 'k--')\n","plt.plot(model.stages[-1].summary.pr.select('recall').collect(),\n","         model.stages[-1].summary.pr.select('precision').collect(),\n","        label='PR curve (area = {:0.2f})'.format(test_PRAUC))\n","plt.xlabel('Recall')\n","plt.ylabel('Precision')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"otLXUTkoE3VI"},"source":["The **training** result is already **very good**, the **test result** is **not much worse**. So, this task is easy for LogisticRegression, even with small vector size. With a grid search we can tune the hyper-parameters to find the optimal settings.\n","\n","With **20 classes**, the **task** gets **harder**, however, which you can explore with a multi-classification.\n"]},{"cell_type":"markdown","metadata":{"id":"86NUMOCNE3VI"},"source":["## Extra Tasks\n","\n","These tasks are a bit more involved in terms of programming.  So, you can try them at home."]},{"cell_type":"markdown","metadata":{"id":"XFwmYotbE3VL"},"source":["### Tuning the Hyper-Parameters\n","\n","The Spark ML package includes a class for Hyper-Parameter tuning, such as **CrossValidator** (for smaller datasets) and **TrainTestValidator** (for larger datasets).\n","\n","Here, we define a grid and the CrossValidator tests over all points in the grid.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VLMXka4WE3VM"},"outputs":[],"source":["# We use a ParamGridBuilder to construct a grid of parameters to search over.\n","\n","# With 1 value for hashingTF.numFeatures and 1 value for idf,\n","# the grid below has only 1 parameter setting for CrossValidator to choose from.\n","#>>> Extend the grid once you are sure that it runs.\n","\n","from pyspark.ml.tuning import ParamGridBuilder\n","\n","paramGrid = ParamGridBuilder()\\\n","    .addGrid(hashingTF.numFeatures, [100])\\\n","    .addGrid(idf.minDocFreq, [10])\\\n","    .build()"]},{"cell_type":"markdown","metadata":{"id":"cMRS6LS-3_i6"},"source":["Training the model (`cvModel.fit`) taks several minutes. Make sure that everything so far has worked correctly,  to avoid spending a long time waiting for an invalid result."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"heyGef0ME3VP"},"outputs":[],"source":["# A CrossValidator requires an Estimator, a set of Estimator ParamMaps, and an Evaluator.\n","from pyspark.ml.tuning import CrossValidator\n","\n","cv = CrossValidator().setEstimator(pipeline).setEvaluator(evaluatorROC).setEstimatorParamMaps(paramGrid).setNumFolds(2)\n","# Note: This takes a long time to run with proper sized grid\n","# Do this step only when you've done everything else first!\n","%time cvModel = cv.fit(train_set)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-PtaZrM_E3VR"},"outputs":[],"source":["# Task:\n","# After you have run the cell above , print both results (with and without cross validation)\n","# Observe the results\n","\n","print (\"Area under ROC curve for non-tuned model:\", evaluatorROC.evaluate(test_predictions))\n","print (\"Area under PR curve for non-tuned model:\", evaluatorPR.evaluate(test_predictions))\n",">>> print(\"Area under the ROC curve for best fitted model =\", evaluatorROC.evaluate(...)) # apply the cvModel to the test_set (like done in 4) for the pipeline model to create 'test_predictions')\n",">>> print(\"Area under the PR curve for best fitted model =\", evaluatorPR.evaluate(...)) # as above\n"]},{"cell_type":"markdown","metadata":{"id":"eBqx7FN_E3VJ"},"source":["### Multi-class classification\n","\n","Use **all the 20 topics** in the dataset as class labels. The reading of the data is straightforward. You will need to use a different mapping from newsgroup names to class labels, though. Then you will need to **reconsider evaluation**, as the ROC AUC is only defined for the binary case.\n","\n","The **perfomance** will be **lower**, so that it is worth to try and **tune** the **hyper-parameters**."]},{"cell_type":"markdown","metadata":{"id":"F1uKRbbmZH6e"},"source":["# Running PySpark on Google's Cloud Platform\n","\n","**Up to now**, we have been running **PySpark** on Google **Colab**. This means our code is executed on a virtual Linux machine hosted by Google. Our code can already run in parallel, but only using the multiple cores on one machine (see CPU info below)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wyNE_KPKZIux"},"outputs":[],"source":["!lscpu | grep \"socket\\|Socket\\|core\\|CPU\""]},{"cell_type":"markdown","metadata":{"id":"KB98GrAhZVa0"},"source":["Next we'll move towards running **PySpark code on multiple machines**, which means that synchronisation between jobs will need to happen over the network instead of in the shared working memory. We'll be using [Cloud Dataproc](https://cloud.google.com/dataproc), a **managed Spark installation** hosted by Google (so it's a **platform-as-a-service**).\n","\n","Open the _Google Cloud Introduction_ pdf file on Moodle (in section 4) and follow the instructions to get started with creating a free trial account and running some example code.\n","\n"]}],"metadata":{"colab":{"private_outputs":true,"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":0}