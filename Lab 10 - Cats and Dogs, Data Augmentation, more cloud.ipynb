{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1voilV4DiG7WOhGVqh-4_RgENGX4Ociw5","timestamp":1582916758670},{"file_id":"https://github.com/google/eng-edu/blob/master/ml/pc/exercises/image_classification_part2.ipynb","timestamp":1553841086914}],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"TPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"GrFpZXNQB9FW"},"source":["# Lab Sheet 10: Cats and dogs image classification with Deep Learning: the effect of artificially augmented data and larger datasets (not needed for the coursework)\n","\n","This notebook is based on an exercise published by Google. Implement the training and move the whole training into the cloud. You can also try to use the whole large dataset.\n","\n","This lab addresses the use of **artifically augmented data** and larger datasets on deep learning models. In addition, there is furtehr practice in **using machine learning in the cloud**."]},{"cell_type":"markdown","metadata":{"id":"R7DeVfQ9a4iH"},"source":["```\n","# Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","#\n","# https://www.apache.org/licenses/LICENSE-2.0\n","#\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License.\n","```"]},{"cell_type":"markdown","metadata":{"id":"YHK6DyunSbs4"},"source":["# Cat vs. Dog Image Classification\n"]},{"cell_type":"markdown","source":["## 1) Loading Image Files and Reducing Overfitting with Dropout and Data Augmentation\n","\n","In this notebook we will build a model to classify cats vs. dogs, and improve accuracy by employing a couple of strategies to reduce overfitting: **data augmentation** and **dropout**.\n","\n","We will follow these steps:\n","\n","1. Explore how data augmentation works by making random transformations to training images.\n","2. Add data augmentation to our data preprocessing.\n","3. Add dropout to the convnet.\n","4. Retrain the model and evaluate loss and accuracy.\n","\n","Let's get started!"],"metadata":{"id":"sRaZjqAHOZPF"}},{"cell_type":"markdown","metadata":{"id":"E3sSwzshfSpE"},"source":["## 2) Exploring Data Augmentation\n","\n","Let's get familiar with the concept of **data augmentation**, an essential way to fight overfitting for computer vision models.\n","\n","In order to make the most of our few training examples, we will \"augment\" them via a number of random transformations, so that at training time, **our model will never see the exact same picture twice**. This helps prevent overfitting and helps the model generalize better.\n","\n","This can be done by configuring a number of random transformations to be performed on the images read by our `ImageDataGenerator` instance. Let's get started with an example:"]},{"cell_type":"code","source":["!pip install tensorflow"],"metadata":{"id":"-DQcNQeGFZy3"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XK-IN_zNgLlT"},"source":["from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","\n","datagen = ImageDataGenerator(\n","    rotation_range=40,\n","    width_shift_range=0.2,\n","    height_shift_range=0.2,\n","    shear_range=0.2,\n","    zoom_range=0.2,\n","    horizontal_flip=True,\n","    fill_mode='nearest'\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ijUDyVZtSgz3"},"source":["These are just a few of the options available (for more, see the [Keras documentation](https://keras.io/preprocessing/image/). Let's quickly go over what we just wrote:\n","\n","- `rotation_range` is a value in degrees (0â€“180), a range within which to randomly rotate pictures.\n","- `width_shift` and `height_shift` are ranges (as a fraction of total width or height) within which to randomly translate pictures vertically or horizontally.\n","- `shear_range` is for randomly applying shearing transformations.\n","- `zoom_range` is for randomly zooming inside pictures.\n","- `horizontal_flip` is for randomly flipping half of the images horizontally. This is relevant when there are no assumptions of horizontal assymmetry (e.g. real-world pictures).\n","- `fill_mode` is the strategy used for filling in newly created pixels, which can appear after a rotation or a width/height shift.\n","\n","Let's take a look at our augmented images. First let's set up our example files, as in Exercise 1.\n"]},{"cell_type":"markdown","metadata":{"id":"grzOIOhoY366"},"source":["**NOTE:** The 2,000 images used in this exercise are excerpted from the [\"Dogs vs. Cats\" dataset](https://www.kaggle.com/c/dogs-vs-cats/data) available on Kaggle, which contains 25,000 images. Here, we use a subset of the full dataset to decrease training time for educational purposes."]},{"cell_type":"code","metadata":{"id":"dhztKtUSFMXp"},"source":["!wget --no-check-certificate \\\n","   https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip -O \\\n","   /tmp/cats_and_dogs_filtered.zip"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LWkSRoJRfvGL"},"source":["import os\n","import zipfile\n","\n","local_zip = '/tmp/cats_and_dogs_filtered.zip'\n","zip_ref = zipfile.ZipFile(local_zip, 'r')\n","zip_ref.extractall('/tmp')\n","zip_ref.close()\n","\n","base_dir = '/tmp/cats_and_dogs_filtered'\n","train_dir = os.path.join(base_dir, 'train')\n","validation_dir = os.path.join(base_dir, 'validation')\n","\n","# Directory with our training cat pictures\n","train_cats_dir = os.path.join(train_dir, 'cats')\n","\n","# Directory with our training dog pictures\n","train_dogs_dir = os.path.join(train_dir, 'dogs')\n","\n","# Directory with our validation cat pictures\n","validation_cats_dir = os.path.join(validation_dir, 'cats')\n","\n","# Directory with our validation dog pictures\n","validation_dogs_dir = os.path.join(validation_dir, 'dogs')\n","\n","train_cat_fnames = os.listdir(train_cats_dir)\n","train_dog_fnames = os.listdir(train_dogs_dir)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"02r1oXaegECk"},"source":["Next, let's apply the `datagen` transformations to a cat image from the training set to produce five random variants. Rerun the cell a few times to see fresh batches of random variants."]},{"cell_type":"code","metadata":{"id":"ap-nt8Byfaov"},"source":["%matplotlib inline\n","\n","import matplotlib.pyplot as plt\n","\n","from tensorflow.keras.preprocessing.image import array_to_img, img_to_array, load_img\n","\n","img_path = os.path.join(train_cats_dir, train_cat_fnames[2])\n","img = load_img(img_path, target_size=(150, 150))  # this is a PIL image\n","x = img_to_array(img)  # Numpy array with shape (150, 150, 3)\n","x = x.reshape((1,) + x.shape)  # Numpy array with shape (1, 150, 150, 3)\n","\n","# The .flow() command below generates batches of randomly transformed images\n","# It will loop indefinitely, so we need to `break` the loop at some point!\n","i = 0\n","for batch in datagen.flow(x, batch_size=1):\n","  plt.figure(i)\n","  imgplot = plt.imshow(array_to_img(batch[0]))\n","  i += 1\n","  if i % 5 == 0:\n","    break"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eywLKduLmYPY"},"source":["## 3) Add Data Augmentation to the Preprocessing Step\n","\n","Now let's add our data-augmentation transformations from [**Exploring Data Augmentation**](#scrollTo=E3sSwzshfSpE) to our data preprocessing configuration:"]},{"cell_type":"code","metadata":{"id":"e8HgwcAbmdcu"},"source":["# Adding rescale, rotation_range, width_shift_range, height_shift_range,\n","# shear_range, zoom_range, and horizontal flip to our ImageDataGenerator\n","train_datagen = ImageDataGenerator(\n","    rescale=1./255,\n","    rotation_range=40,\n","    width_shift_range=0.2,\n","    height_shift_range=0.2,\n","    shear_range=0.2,\n","    zoom_range=0.2,\n","    horizontal_flip=True,)\n","\n","# Note that the validation data should not be augmented!\n","test_datagen = ImageDataGenerator(rescale=1./255)\n","\n","# Flow training images in batches of 20 using train_datagen generator\n","train_generator = train_datagen.flow_from_directory(\n","        train_dir,  # This is the source directory for training images\n","        target_size=(150, 150),  # All images will be resized to 150x150\n","        batch_size=20,\n","        # Since we use binary_crossentropy loss, we need binary labels\n","        class_mode='binary')\n","\n","# Flow validation images in batches of 20 using test_datagen generator\n","validation_generator = test_datagen.flow_from_directory(\n","        validation_dir,\n","        target_size=(150, 150),\n","        batch_size=20,\n","        class_mode='binary')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"K-3PrfDwDJjB"},"source":["If we train a new network using this data augmentation configuration, our network will never see the same input twice. However the inputs that it sees are still heavily intercorrelated, so this might not be quite enough to completely get rid of overfitting."]},{"cell_type":"markdown","metadata":{"id":"lYguAfH3gyv6"},"source":["## 4) Adding Dropout\n","\n","Another popular strategy for fighting overfitting is to use **dropout**."]},{"cell_type":"markdown","metadata":{"id":"VtDl3oEZo_7Z"},"source":["**TIP:** To learn more about dropout, see [Training Neural Networks](https://developers.google.com/machine-learning/crash-course/training-neural-networks/video-lecture) in [Machine Learning Crash Course](https://developers.google.com/machine-learning/crash-course/)."]},{"cell_type":"markdown","metadata":{"id":"bi3c0YtwpRUr"},"source":["Let's reconfigure our convnet architecture from Exercise 1 to add some dropout, right before the final classification layer:"]},{"cell_type":"code","metadata":{"id":"SVC4FgxiDje6"},"source":["from tensorflow.keras import layers\n","from tensorflow.keras import Model\n","from tensorflow.keras.optimizers import RMSprop\n","\n","# Our input feature map is 150x150x3: 150x150 for the image pixels, and 3 for\n","# the three color channels: R, G, and B\n","img_input = layers.Input(shape=(150, 150, 3))\n","\n","# First convolution extracts 16 filters that are 3x3\n","# Convolution is followed by max-pooling layer with a 2x2 window\n","x = layers.Conv2D(16, 3, activation='relu')(img_input)\n","x = layers.MaxPooling2D(2)(x)\n","\n","# Second convolution extracts 32 filters that are 3x3\n","# Convolution is followed by max-pooling layer with a 2x2 window\n","x = layers.Conv2D(32, 3, activation='relu')(x)\n","x = layers.MaxPooling2D(2)(x)\n","\n","# Third convolution extracts 64 filters that are 3x3\n","# Convolution is followed by max-pooling layer with a 2x2 window\n","x = layers.Convolution2D(64, 3, activation='relu')(x)\n","x = layers.MaxPooling2D(2)(x)\n","\n","# Flatten feature map to a 1-dim tensor\n","x = layers.Flatten()(x)\n","\n","# Create a fully connected layer with ReLU activation and 512 hidden units\n","x = layers.Dense(512, activation='relu')(x)\n","\n","# Add a dropout rate of 0.5\n","x = layers.Dropout(0.5)(x)\n","\n","# Create output layer with a single node and sigmoid activation\n","output = layers.Dense(1, activation='sigmoid')(x)\n","\n","# Configure and compile the model\n","model = Model(img_input, output)\n","model.compile(loss='binary_crossentropy',\n","              optimizer=RMSprop(learning_rate=0.001),\n","              metrics=['acc'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jKSgmOt5itEF"},"source":["## 5) Retrain the Model\n","\n","With data augmentation and dropout in place, let's retrain our convnet model. This time, let's train on all 2,000 images available, for 30 epochs, and validate on all 1,000 test images. (This may take a few minutes to run.) See if you can write the code yourself. The [Keras API reference](https://keras.io/models/sequential/) might come in handy.\n"]},{"cell_type":"code","metadata":{"cellView":"code","id":"VWr-MDk4ksJr"},"source":["#>>> WRITE CODE TO TRAIN THE MODEL ON ALL 2000 IMAGES FOR 30 EPOCHS, AND VALIDATE  ON ALL 1,000 TEST IMAGES"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1LTWMLV6SUvP"},"source":["Note that with data augmentation in place, the 2,000 training images are randomly transformed each time a new training epoch runs, which means that the model will never see the same image twice during training."]},{"cell_type":"markdown","metadata":{"id":"IZqvC9UJlWc2"},"source":["## 6) Evaluate the Results\n","\n","Let's evaluate the results of model training with data augmentation and dropout:"]},{"cell_type":"code","metadata":{"id":"NKCjHegASXaA"},"source":["# Retrieve a list of accuracy results on training and test data\n","# sets for each training epoch\n","acc = history.history['acc']\n","val_acc = history.history['val_acc']\n","\n","# Retrieve a list of list results on training and test data\n","# sets for each training epoch\n","loss = history.history['loss']\n","val_loss = history.history['val_loss']\n","\n","# Get number of epochs\n","epochs = range(len(acc))\n","\n","# Plot training and validation accuracy per epoch\n","plt.plot(epochs, acc)\n","plt.plot(epochs, val_acc)\n","plt.title('Training and validation accuracy')\n","plt.legend(['training', 'validation'], loc='lower right')\n","plt.figure()\n","\n","# Plot training and validation loss per epoch\n","plt.plot(epochs, loss)\n","plt.plot(epochs, val_loss)\n","plt.title('Training and validation loss')\n","plt.legend(['training', 'validation'], loc='upper right')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ej-s7-_eShij"},"source":["Interpret the results in terms of overfitting and accuracy improvements and the learning process."]},{"cell_type":"code","metadata":{"id":"uaby2j4wr3rS"},"source":[],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QrtB6e_P-xhI"},"source":["*write your answer here*"]},{"cell_type":"markdown","metadata":{"id":"LG_arSq0njtU"},"source":["## 7) AI in the cloud\n","Port the training into the Google Cloud AI Platform. Using the same implementation patterns as before, this is a larger task. You can explore using different machine types and can try using the full dataset.\n","\n","We start with the usual set-up and mount our Google Drive."]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","%cd \"/content/drive/MyDrive\"\n","!mkdir BD\n","!mkdir BD/lab10"],"metadata":{"id":"Om7YefjSL4UT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Then we need to create the directory for our code package for the cloud. It needs to contain a file `__init__.py`, which can be empty. You can use the command line tool `touch` to create it. Look [here](https://www.man7.org/linux/man-pages/man1/touch.1.html) for how it works."],"metadata":{"id":"AhS-z9lnL5tc"}},{"cell_type":"code","source":["%cd \"/content/drive/MyDrive/BD/lab10\"\n","!mkdir trainer\n",">>> ### use !touch to create a file __init__.py in the trainer directory\n","!ls -lh trainer"],"metadata":{"id":"YjRgNyAwOlkm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Then we authenticate with the Google Cloud."],"metadata":{"id":"bBTowp3RMte0"}},{"cell_type":"code","source":["import sys\n","if 'google.colab' in sys.modules:\n","    from google.colab import auth\n","    auth.authenticate_user()"],"metadata":{"id":"DDCDBuZ-MyOh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Then we need to specify our project and store the name in a Python variable (which can be used in the shell code -- that starts with '!' -- by prepending a '\\$' sign like this: '$PROJECT')."],"metadata":{"id":"tGcVpAXV7za3"}},{"cell_type":"code","source":["PROJECT = 'my-project' ### USE YOUR OWN PROJECT ID HERE! ###\n",">>>!gcloud config set project ### SET THE PROJECT NAME ###\n","REGION = 'us-central1' # for use later with ai-platform\n","!gcloud config list  # show some information"],"metadata":{"id":"7trg6sve7z8c"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Next, we make sure we have a storage bucket to save our results in. We can't use it for the training data in this lab, because the ImageDataGenerator doesn't read from cloud storage."],"metadata":{"id":"0vwdFnEC8xk0"}},{"cell_type":"code","source":["BUCKET = 'gs://{}-storage'.format(PROJECT)\n","!gsutil # make the bucket"],"metadata":{"id":"oGbyed8c87lr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"T0qtvcmjsYDF"},"source":["Then we need to combine all relevant code into a cell and write it into a file that can be run with the AI Platform."]},{"cell_type":"code","metadata":{"id":"OcOtSL60scla"},"source":["#%% writefile trainer/task.py\n",">>>### Copy the code from above here and adapt it to run here."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["First, we can run the code directly from the cell (not writing it to a file) and it should behave as above.\n","\n","Then you can write it to a file (uncommenting the first line of the cell) and then use `%run trainer/task.py` in a code cell. The behaviour should be the same as before. The most common problem is not including all code, which will now become noticable as the file is executed outside the notebook."],"metadata":{"id":"iTRl6LfsNvk0"}},{"cell_type":"code","metadata":{"id":"HilDR7CRoqAa"},"source":["# try it here"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tqy9K0iuswR7"},"source":["Next, we can then test the training locally using the `local train` mode of the gcloud tool. You can see [here](https://cloud.google.com/sdk/gcloud/reference/ai-platform/local/train) how to set the missing parameters."]},{"cell_type":"code","metadata":{"id":"hJjGd7zps8lR"},"source":["TRAINER_PACKAGE_PATH=\"trainer\"\n","MAIN_TRAINER_MODULE=\"trainer.task\"\n","BUCKET = \"gs://bd-labs-test-storage\"\n","PACKAGE_STAGING_PATH=BUCKET\n","import datetime\n","NOW=datetime.datetime.now().strftime(\"%y%m%d_%H%M\")\n","JOB_NAME = \"bd_labs_job_\"+NOW\n","JOB_DIR=BUCKET+'/jobs/'+JOB_NAME\n","\n","!gcloud ai-platform local train \\\n",">>># ... set the --job-dir, --package-path, and --module-name here\n","    -- --config standard_gpu --batch-size 32"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BDIujCp4ti_V"},"source":["If that worked out, we can next move to the AI Platform, but need some modifications first.\n","\n","You need to download the data to the machine where, to make it accessible for the AI Platform, and adapt your code. You can use the code below to achieve that.\n","\n","Test the code again locally after modifiction.\n"]},{"cell_type":"code","metadata":{"id":"z8e3UWFctvsT"},"source":["### We need to copy the data over to the AI Platform Machine first, because the\n","# ImageDataGenerator.flow_from_directory can't read from a Bucket.\n","# Using 'subprocess', as in the Coursework notebook, for getting and unzipping the data.\n","\n","# This needs to go into your code, you should be able to work out where.\n","import subprocess\n","proc = subprocess.run([\"wget\",\"--no-check-certificate\", \"https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip\",\n","                       \"-O\", \"/tmp/cats_and_dogs_filtered.zip\"], stderr=subprocess.PIPE)\n","print(\"wget returned: \" + str(proc.returncode))\n","print(str(proc.stderr))\n","local_zip = '/tmp/cats_and_dogs_filtered.zip'\n","# after this the code from above will work on external machines."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jNShqaOktbPU"},"source":["Finally, train in the cloud using `gcloud ai-platform job submit training`. You can look up [here](https://cloud.google.com/sdk/gcloud/reference/ai-platform/jobs/submit/training#--parameter-server-count) how to set the parameters. Next, have a look at the [Cloud cosole](https://console.cloud.google.com/ai-platform/jobs) to monitor job execution. (Unfortunately, it takes several minutes for the jobs to be started, so try this only after all steps above were successful.)"]},{"cell_type":"code","metadata":{"id":"wy8X18Jktf3A"},"source":["TRAINER_PACKAGE_PATH=\"trainer\"\n","MAIN_TRAINER_MODULE=\"trainer.task\"\n","import datetime\n","NOW=datetime.datetime.now().strftime(\"%y%m%d_%H%M\")\n","JOB_NAME = \"bd_labs_job_\"+NOW\n","BUCKET = \"gs://bd-labs-test-storage\"\n","PACKAGE_STAGING_PATH=BUCKET\n","JOB_DIR=BUCKET+'/jobs/'+JOB_NAME\n","\n","!gcloud ai-platform jobs submit training $JOB_NAME \\\n","# set --staging-bucket --job-dir --region --package-path --module-name\n","    --runtime-version 2.3 \\\n","    --python-version 3.7 \\\n","    --scale-tier custom \\\n","    --master-machine-type standard_gpu"],"execution_count":null,"outputs":[]}]}